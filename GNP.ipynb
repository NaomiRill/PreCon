{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163d27e1-0259-4883-bb19-a0aefe6eeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Arnoldi():\n",
    "    def build(self, A, v0=None, m=100):\n",
    "\n",
    "        n = A.shape[0]\n",
    "        if v0 is None:\n",
    "            v0 = torch.normal(0, 1, size=(n,), dtype=A.dtype).to(A.device)\n",
    "        beta = torch.linalg.norm(v0)\n",
    "\n",
    "        V = torch.zeros(n, m + 1, dtype=A.dtype).to(A.device)\n",
    "        H = torch.zeros(m + 1, m, dtype=A.dtype).to(A.device)\n",
    "\n",
    "        V[:, 0] = v0 / beta\n",
    "        for j in range(m):\n",
    "            w = A @ V[:, j]\n",
    "            for k in range(j + 1):\n",
    "                H[k, j] = torch.dot(V[:, k], w)\n",
    "                w = w - H[k, j] * V[:, k]\n",
    "            H[j + 1, j] = torch.linalg.norm(w)\n",
    "            V[:, j + 1] = w / H[j + 1, j]\n",
    "\n",
    "        Vm1 = V\n",
    "        barHm = H\n",
    "        return Vm1, barHm\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# The following class implements a streaming dataset, which, in\n",
    "# combined use with the dataloader, produces x of size (n,\n",
    "# batch_size). x is float64 and stays in cpu. It will be moved to the\n",
    "# device and cast to a lower precision for training.\n",
    "class StreamingDataset(IterableDataset):\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, batch_size, training_data, m):\n",
    "        super().__init__()\n",
    "        self.n = A.shape[0]\n",
    "        self.m = m\n",
    "        self.batch_size = batch_size\n",
    "        self.training_data = training_data\n",
    "\n",
    "        # Computations done in device\n",
    "        if training_data == 'x_subspace' or training_data == 'x_mix':\n",
    "            arnoldi = Arnoldi()\n",
    "            Vm1, barHm = arnoldi.build(A, m=m)\n",
    "            W, S, Zh = torch.linalg.svd(barHm, full_matrices=False)\n",
    "            Q = (Vm1[:, :-1] @ Zh.T) / S.view(1, m)\n",
    "            self.Q = Q.to('cpu')\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "\n",
    "            # Computation done in cpu\n",
    "            if self.training_data == 'x_normal':\n",
    "\n",
    "                x = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_subspace':\n",
    "\n",
    "                e = torch.normal(0, 1, size=(self.m, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_mix':\n",
    "\n",
    "                batch_size1 = self.batch_size // 2\n",
    "                e = torch.normal(0, 1, size=(self.m, batch_size1),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                batch_size2 = self.batch_size - batch_size1\n",
    "                x2 = torch.normal(0, 1, size=(self.n, batch_size2),\n",
    "                                  dtype=torch.float64)\n",
    "                x = torch.cat([x, x2], dim=1)\n",
    "                yield x\n",
    "\n",
    "            else:  # self.training_data == 'no_x'\n",
    "\n",
    "                b = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.generate())\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Graph neural preconditioner\n",
    "class GNP():\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, training_data, m, net, device):\n",
    "        self.A = A\n",
    "        self.training_data = training_data\n",
    "        self.m = m\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.dtype = net.dtype\n",
    "\n",
    "    def train(self, batch_size, grad_accu_steps, epochs, optimizer,\n",
    "              scheduler=None, num_workers=0, checkpoint_prefix_with_path=None,\n",
    "              progress_bar=True):\n",
    "\n",
    "        self.net.train()\n",
    "        optimizer.zero_grad()\n",
    "        dataset = StreamingDataset(self.A, batch_size,\n",
    "                                   self.training_data, self.m)\n",
    "        loader = DataLoader(dataset, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        hist_loss = []\n",
    "        best_loss = np.inf\n",
    "        best_epoch = -1\n",
    "        checkpoint_file = None\n",
    "\n",
    "        if progress_bar:\n",
    "            pbar = tqdm(total=epochs, desc='Train')\n",
    "\n",
    "        for epoch, x_or_b in enumerate(loader):\n",
    "\n",
    "            # Generate training data\n",
    "            if self.training_data != 'no_x':\n",
    "                x = x_or_b[0].to(self.device)\n",
    "                b = self.A @ x\n",
    "                b, x = b.to(self.dtype), x.to(self.dtype)\n",
    "            else:  # self.training_data == 'no_x'\n",
    "                b = x_or_b[0].to(self.device).to(self.dtype)\n",
    "\n",
    "            # Train\n",
    "            x_out = self.net(b)\n",
    "            b_out = (self.A @ x_out.to(torch.float64)).to(self.dtype)\n",
    "            loss = F.l1_loss(b_out, b)\n",
    "\n",
    "            # Bookkeeping\n",
    "            hist_loss.append(loss.item())\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_epoch = epoch\n",
    "                if checkpoint_prefix_with_path is not None:\n",
    "                    checkpoint_file = checkpoint_prefix_with_path + 'best.pt'\n",
    "                    torch.save(self.net.state_dict(), checkpoint_file)\n",
    "\n",
    "            # Train (cont.)\n",
    "            loss.backward()\n",
    "            if (epoch + 1) % grad_accu_steps == 0 or epoch == epochs - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Bookkeeping (cont.)\n",
    "            if progress_bar:\n",
    "                pbar.set_description(f'Train loss {loss:.1e}')\n",
    "                pbar.update()\n",
    "            if epoch == epochs - 1:\n",
    "                break\n",
    "\n",
    "        # Bookkeeping (cont.)\n",
    "        if checkpoint_file is not None:\n",
    "            checkpoint_file_old = checkpoint_file\n",
    "            checkpoint_file = \\\n",
    "                checkpoint_prefix_with_path + f'epoch_{best_epoch}.pt'\n",
    "            os.rename(checkpoint_file_old, checkpoint_file)\n",
    "\n",
    "        return hist_loss, best_loss, best_epoch, checkpoint_file\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, r):  # r: float64\n",
    "        self.net.eval()\n",
    "        # r = r.to(self.dtype)  # -> lower precision\n",
    "        r = r.view(-1, 1)\n",
    "        z = self.net(r)\n",
    "        z = z.view(-1)\n",
    "        # z = z.double()  # -> float64\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7b660-fdfa-445f-8eb3-2fad8e85b4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NaomiKernel)",
   "language": "python",
   "name": "naomikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
