{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b64692-997c-43c4-a64a-7c9806cc9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data_generator import create_fesom_matrix\n",
    "from GNP import GNP, scale_A_by_spectral_radius\n",
    "from ResGCN import ResGCN\n",
    "from GMRES import GMRES\n",
    "from scipy.sparse import csc_matrix, identity\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a686bff-0227-4d92-9f8f-dcce6c0b227a",
   "metadata": {},
   "source": [
    "# Hierarchichal training\n",
    "\n",
    "In our case we can make problem gradually more difficult to solve, while keeping the the `A` matrix in exactly the same shape. This should allow us to use the same `ResGCN`\n",
    "\n",
    "- First way is to increase the scale of the filter, as the bigger scacle the worse is the conditioning of the matrix. \n",
    "- Second is increasing the order of the filer `n`, which makes much more difficult to solve the problem\n",
    "---\n",
    "\n",
    "To start we have to generate sythetic problem of given size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbccb2fd-7382-4e87-bfc1-28a0b77d7b12",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/fesom.mesh.diag.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:219\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[39m, in \u001b[36mLRUCache.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.move_to_end(key)\n",
      "\u001b[31mKeyError\u001b[39m: [<class 'netCDF4._netCDF4.Dataset'>, ('/path/to/fesom.mesh.diag.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '5b8b244b-4d88-4402-9f32-f208c3d3d684']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m SST_PATH  = \u001b[33m\"\u001b[39m\u001b[33m/path/to/sst.fesom.2000.nc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Load mesh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m mesh = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMESH_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Coordinates (1D arrays of node positions)\u001b[39;00m\n\u001b[32m     18\u001b[39m xcoord = mesh[\u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/api.py:596\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    585\u001b[39m     decode_cf,\n\u001b[32m    586\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    592\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    593\u001b[39m )\n\u001b[32m    595\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    603\u001b[39m     backend_ds,\n\u001b[32m    604\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m     **kwargs,\n\u001b[32m    616\u001b[39m )\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:744\u001b[39m, in \u001b[36mNetCDF4BackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_dataset\u001b[39m(\n\u001b[32m    723\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    724\u001b[39m     filename_or_obj: T_PathFileOrDataStore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    741\u001b[39m     autoclose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    742\u001b[39m ) -> Dataset:\n\u001b[32m    743\u001b[39m     filename_or_obj = _normalize_path(filename_or_obj)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     store = \u001b[43mNetCDF4DataStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m     store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:524\u001b[39m, in \u001b[36mNetCDF4DataStore.open\u001b[39m\u001b[34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    521\u001b[39m     manager = CachingFileManager(\n\u001b[32m    522\u001b[39m         netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n\u001b[32m    523\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:428\u001b[39m, in \u001b[36mNetCDF4DataStore.__init__\u001b[39m\u001b[34m(self, manager, group, mode, lock, autoclose)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m._group = group\n\u001b[32m    427\u001b[39m \u001b[38;5;28mself\u001b[39m._mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28mself\u001b[39m.format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m.data_model\n\u001b[32m    429\u001b[39m \u001b[38;5;28mself\u001b[39m._filename = \u001b[38;5;28mself\u001b[39m.ds.filepath()\n\u001b[32m    430\u001b[39m \u001b[38;5;28mself\u001b[39m.is_remote = is_remote_uri(\u001b[38;5;28mself\u001b[39m._filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:533\u001b[39m, in \u001b[36mNetCDF4DataStore.ds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:527\u001b[39m, in \u001b[36mNetCDF4DataStore._acquire\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:207\u001b[39m, in \u001b[36mCachingFileManager.acquire_context\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[T_File]:\n\u001b[32m    206\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     file, cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:225\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    223\u001b[39m     kwargs = kwargs.copy()\n\u001b[32m    224\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2517\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Dataset.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2154\u001b[39m, in \u001b[36mnetCDF4._netCDF4._ensure_nc_success\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/path/to/fesom.mesh.diag.nc'"
     ]
    }
   ],
   "source": [
    "# --- FESOM data setup (replaces synthetic) ---\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "\n",
    "# device (only if you don't already define it elsewhere)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "MESH_PATH = \"/path/to/fesom.mesh.diag.nc\"\n",
    "SST_PATH  = \"/path/to/sst.fesom.2000.nc\"\n",
    "\n",
    "# Load mesh\n",
    "mesh = xr.open_dataset(MESH_PATH)\n",
    "\n",
    "# Coordinates (1D arrays of node positions)\n",
    "xcoord = mesh[\"lon\"].values\n",
    "ycoord = mesh[\"lat\"].values\n",
    "\n",
    "# Triangles: FESOM meshes are typically 1-based; shift to 0-based for numpy/mpl\n",
    "# face_nodes usually has shape (3, ntri) â€” transpose to (ntri, 3)\n",
    "tri = mesh[\"face_nodes\"].T.values - 1\n",
    "\n",
    "# Optionally build any precomputed matrices/indices you need\n",
    "from synthetic_data_generator import create_fesom_matrix\n",
    "ss, ii, jj, tri, xcoord, ycoord = create_fesom_matrix(xcoord, ycoord, tri)\n",
    "\n",
    "# Mask triangles that wrap the dateline for cleaner plotting\n",
    "ok = (xcoord[tri].max(axis=1) - xcoord[tri].min(axis=1)) < 10\n",
    "triang = mpl.tri.Triangulation(xcoord, ycoord, tri[ok, :])\n",
    "\n",
    "# Dimensionality (number of 2D surface nodes)\n",
    "n2d = xcoord.shape[0]\n",
    "\n",
    "# Load SST and pick a single time slice as the target vector\n",
    "ds = xr.open_dataset(SST_PATH)\n",
    "\n",
    "# pick the variable (change 'sst' if your file uses a different name)\n",
    "var_name = \"sst\" if \"sst\" in ds.data_vars else list(ds.data_vars)[0]\n",
    "data_da = ds[var_name]        # expected shape: (time, node)\n",
    "\n",
    "time_index = 0                # choose which timestep to use\n",
    "tt = data_da.isel(time=time_index).values  # numpy array, shape (n2d,)\n",
    "\n",
    "# Torch tensor expected downstream\n",
    "data = torch.tensor(tt, device=device, dtype=torch.float64)\n",
    "\n",
    "# --- End FESOM data setup ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b74427e-577b-4f64-80dd-73174beb2b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/fesom.mesh.diag.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:219\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[39m, in \u001b[36mLRUCache.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.move_to_end(key)\n",
      "\u001b[31mKeyError\u001b[39m: [<class 'netCDF4._netCDF4.Dataset'>, ('/path/to/fesom.mesh.diag.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '3ab5615c-c0d5-4515-993d-a1bada5e7cc7']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m SST_PATH  = \u001b[33m\"\u001b[39m\u001b[33m/path/to/sst.fesom.2000.nc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load mesh and build triangulation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m mesh = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMESH_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m xcoord = mesh[\u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m     11\u001b[39m ycoord = mesh[\u001b[33m'\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/api.py:596\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    585\u001b[39m     decode_cf,\n\u001b[32m    586\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    592\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    593\u001b[39m )\n\u001b[32m    595\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m backend_ds = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m ds = _dataset_from_backend_dataset(\n\u001b[32m    603\u001b[39m     backend_ds,\n\u001b[32m    604\u001b[39m     filename_or_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m     **kwargs,\n\u001b[32m    616\u001b[39m )\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:744\u001b[39m, in \u001b[36mNetCDF4BackendEntrypoint.open_dataset\u001b[39m\u001b[34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_dataset\u001b[39m(\n\u001b[32m    723\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    724\u001b[39m     filename_or_obj: T_PathFileOrDataStore,\n\u001b[32m   (...)\u001b[39m\u001b[32m    741\u001b[39m     autoclose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    742\u001b[39m ) -> Dataset:\n\u001b[32m    743\u001b[39m     filename_or_obj = _normalize_path(filename_or_obj)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     store = \u001b[43mNetCDF4DataStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m     store_entrypoint = StoreBackendEntrypoint()\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:524\u001b[39m, in \u001b[36mNetCDF4DataStore.open\u001b[39m\u001b[34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    521\u001b[39m     manager = CachingFileManager(\n\u001b[32m    522\u001b[39m         netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n\u001b[32m    523\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:428\u001b[39m, in \u001b[36mNetCDF4DataStore.__init__\u001b[39m\u001b[34m(self, manager, group, mode, lock, autoclose)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m._group = group\n\u001b[32m    427\u001b[39m \u001b[38;5;28mself\u001b[39m._mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28mself\u001b[39m.format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m.data_model\n\u001b[32m    429\u001b[39m \u001b[38;5;28mself\u001b[39m._filename = \u001b[38;5;28mself\u001b[39m.ds.filepath()\n\u001b[32m    430\u001b[39m \u001b[38;5;28mself\u001b[39m.is_remote = is_remote_uri(\u001b[38;5;28mself\u001b[39m._filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:533\u001b[39m, in \u001b[36mNetCDF4DataStore.ds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:527\u001b[39m, in \u001b[36mNetCDF4DataStore._acquire\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:207\u001b[39m, in \u001b[36mCachingFileManager.acquire_context\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[T_File]:\n\u001b[32m    206\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     file, cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    209\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/NaomiKernel/lib/python3.12/site-packages/xarray/backends/file_manager.py:225\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    223\u001b[39m     kwargs = kwargs.copy()\n\u001b[32m    224\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2517\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Dataset.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/netCDF4/_netCDF4.pyx:2154\u001b[39m, in \u001b[36mnetCDF4._netCDF4._ensure_nc_success\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/path/to/fesom.mesh.diag.nc'"
     ]
    }
   ],
   "source": [
    "# FESOM data setup (replaces synthetic)\n",
    "import xarray as xr\n",
    "\n",
    "# Set your data paths:\n",
    "MESH_PATH = \"/path/to/fesom.mesh.diag.nc\"\n",
    "SST_PATH  = \"/path/to/sst.fesom.2000.nc\"\n",
    "\n",
    "# Load mesh and build triangulation\n",
    "mesh = xr.open_dataset(MESH_PATH)\n",
    "xcoord = mesh['lon'].values\n",
    "ycoord = mesh['lat'].values\n",
    "tri = mesh['face_nodes'].T.values - 1\n",
    "\n",
    "from synthetic_data_generator import create_fesom_matrix\n",
    "ss, ii, jj, tri, xcoord, ycoord = create_fesom_matrix(xcoord, ycoord, tri)\n",
    "\n",
    "# mask triangles that wrap the dateline for clean plotting\n",
    "ok = (xcoord[tri].max(axis=1) - xcoord[tri].min(axis=1)) < 10\n",
    "triang = mpl.tri.Triangulation(xcoord, ycoord, tri[ok, :])\n",
    "\n",
    "# Dimensionality\n",
    "n2d = xcoord.shape[0]\n",
    "\n",
    "# Load SST data and pick a single time slice as the target vector\n",
    "ds = xr.open_dataset(SST_PATH)\n",
    "data_da = ds['sst']               # shape: (time, node)\n",
    "time_index = 0                    # choose which timestep to use\n",
    "tt = data_da.isel(time=time_index).values  # numpy array (n2d,)\n",
    "\n",
    "# If later code expects a torch tensor called `data`\n",
    "data = torch.tensor(tt, device=device, dtype=torch.float64)\n",
    "# End FESOM data setup\n",
    "\n",
    "Lx = 1000 \n",
    "dxm = 2 \n",
    "n2d = np.arange(0, Lx + 1, dxm, dtype=\"float32\").shape[0]**2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cef99-d9bb-495e-862d-26d070b75ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "triang = mpl.tri.Triangulation(xcoord, ycoord, tri)\n",
    "\n",
    "plt.tripcolor(triang, tt)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484b2ea-4f2e-4764-9573-46674ed60689",
   "metadata": {},
   "source": [
    "Now lets define model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce490d-9a70-46e0-8a43-6da440898f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8              # number of layers in GNP\n",
    "embed = 32                  # embedding dimension in GNP\n",
    "hidden = 64                 # hidden dimension in MLPs in GNP\n",
    "drop_rate = 0.05            # dropout rate in GNP\n",
    "disable_scale_input = False # whether disable the scaling of inputs in GNP\n",
    "dtype = torch.float64       # training precision for GNP\n",
    "lr = 2e-3                   # learning rate in training GNP\n",
    "weight_decay = 0.0          # weight decay in training GNP\n",
    "training_data = 'x_mix'     # type of training data x\n",
    "m = 80                      # Krylov subspace dimension for training data\n",
    "batch_size = 4              # batch size in training GNP\n",
    "grad_accu_steps = 1         # gradient accumulation steps in training GNP\n",
    "epochs = 1000               # number of epochs in training GNP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4747d-3b4f-499e-b3ed-d9a3dc84c2aa",
   "metadata": {},
   "source": [
    "Now we need to create scales that we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48307993-42bf-4328-a301-da0bacb9d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kc = 2 * math.pi / np.logspace(1, 3, 10) # 10 values from 10 to 1000 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c0938-2f77-4dca-9249-13b319fb2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e625cc0-b0df-4d30-90a8-263a56722dd5",
   "metadata": {},
   "source": [
    "Now we define the matricies for model to learn preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb2d9c-1d0f-447a-b192-2493333a3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1 # Filter order\n",
    "device = torch.device(\"cuda\")\n",
    "solver = GMRES()\n",
    "As = []\n",
    "\n",
    "for k in kc: # Looping over the scales\n",
    "    Smat1 = csc_matrix((ss * (1.0 / np.square(k)), (ii, jj)), shape=(n2d, n2d))\n",
    "    Smat = identity(n2d) + 2.0 * (Smat1 ** n)\n",
    "    A = torch.sparse_csc_tensor(Smat.indptr, Smat.indices, Smat.data, Smat.shape, dtype=torch.float64).to(device)\n",
    "    A = scale_A_by_spectral_radius(A)\n",
    "    As += [A]\n",
    "    \n",
    "data = torch.tensor(np.array(tt), device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41d60b-3a4e-4360-878a-9efa57dde280",
   "metadata": {},
   "source": [
    "Now model can be initialized for the first phase of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7368996-edd5-45ec-ae83-3ef91db04499",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResGCN(As[-1], num_layers, embed, hidden, drop_rate, scale_input=True, dtype=dtype).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = None\n",
    "M = GNP(A, training_data, m, net, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140d1a5-a3e5-47e1-90c7-726f6763ab6a",
   "metadata": {},
   "source": [
    "As a test we will solve the problem without preconditioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2df9f-7e4c-4e7d-9d51-19225004562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.zeros(len(As))\n",
    "\n",
    "for i in range(len(As)):\n",
    "    ttw = data - As[i] @ data  # Work with perturbations\n",
    "    x, iters, _, _, _ = solver.solve(A=As[i], b=ttw, rtol=1e-6, max_iters=19999, progress_bar=True)\n",
    "    print(\"Iteration \" + str(iters))\n",
    "    iterations[i] = iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab364e62-1e85-45fd-8362-98b011b26516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.logspace(1, 3, 10), iterations)\n",
    "plt.xlabel(\"Length [km]\")\n",
    "plt.ylabel(\"Iterations\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96707a8-58d2-43db-9b10-086a74fdf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTUNA Installation cell required to run notebook in different environments and kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaed98e-cb25-4ca8-877e-9e963233128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna\n",
    "%pip install --upgrade jupyter jupyterlab notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d8ea-4dc4-4cb3-add2-c33834c6aaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384022f-879a-4b5b-9f58-5f701da1997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTUNA AND TASK 1&2 First Try\n",
    "\n",
    "### How to vary phases and vary A(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402078ce-b9b6-4a89-a497-59465ba57143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Optuna objective with Tasks 1&2 (fixed 8 phases, FAST) ===\n",
    "import gc, time, optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _free_cuda():\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "def _dtype_from_name(name: str):\n",
    "    return torch.float64 if \"64\" in name else torch.float32\n",
    "\n",
    "# Choose GROWTH\n",
    "_GROWTH_E = 1.06\n",
    "_GROWTH_M = 1.06\n",
    "_GROWTH_B = 1.06\n",
    "\n",
    "# Choose PHASES and EPOCHS and _AS_SUBSET for variaton\n",
    "# NOT included in hyperparameter tuning, as significant computation time increase for non-guaranteed speed-up\n",
    "# each phase trains on the first _AS_SUBSET graphs; set _AS_SUBSET = len(As) to use all graphs each phase\n",
    "_AS_SUBSET = 2\n",
    "_MAX_PHASES = 2         \n",
    "_CAP_EPOCHS = 200         \n",
    "_CAP_M      = 80        \n",
    "_CAP_BATCH  = 4          \n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # Vary ALL\n",
    "    num_layers_          = trial.suggest_int(\"num_layers\", 4, 16, step=2)\n",
    "    embed_               = trial.suggest_categorical(\"embed\", [16, 32, 48, 64])\n",
    "    hidden_              = trial.suggest_categorical(\"hidden\", [64, 96, 128, 192, 256])\n",
    "    drop_rate_           = trial.suggest_float(\"drop_rate\", 0.0, 0.4, step=0.05)\n",
    "    disable_scale_input_ = trial.suggest_categorical(\"disable_scale_input\", [False, True])\n",
    "    dtype_name_          = trial.suggest_categorical(\"dtype\", [\"float32\", \"float64\"])\n",
    "    lr_                  = trial.suggest_float(\"lr\", 5e-4, 5e-3, log=True)\n",
    "    weight_decay_        = trial.suggest_float(\"weight_decay\", 0.0, 1e-2, log=False)  # allow exact 0\n",
    "    training_data_       = trial.suggest_categorical(\"training_data\", [\"x_mix\", \"x_subspace\", \"x_random\"])\n",
    "    m_                   = trial.suggest_int(\"m\", 20, 80, step=10)\n",
    "    batch_size_          = trial.suggest_categorical(\"batch_size\", [1, 2, 4])\n",
    "    grad_accu_steps_     = trial.suggest_categorical(\"grad_accu_steps\", [1, 2, 4])\n",
    "    epochs_              = trial.suggest_int(\"epochs\", 60, 200)  # serves as BASE epochs\n",
    "    phases_              = trial.suggest_int(\"phases\", 1, _MAX_PHASES)\n",
    "    dtype_ = _dtype_from_name(dtype_name_)\n",
    "\n",
    "    # keep float64 trials lighter (CHATGPT)\n",
    "    if dtype_ == torch.float64:\n",
    "        m_ = min(m_, 40)\n",
    "        batch_size_ = min(batch_size_, 2)\n",
    "\n",
    "    # build model once in the sampled dtype; init on smallest graph\n",
    "    A0 = As[0]\n",
    "    net = ResGCN(\n",
    "        A0,\n",
    "        num_layers=num_layers_, embed=embed_, hidden=hidden_,\n",
    "        drop_rate=drop_rate_, scale_input=(not disable_scale_input_), dtype=dtype_\n",
    "    ).to(device).to(dtype_)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr_, weight_decay=weight_decay_)\n",
    "\n",
    "    M_tune = GNP(A0, training_data_, m_, net, device)\n",
    "    if hasattr(M_tune, \"dtype\"):\n",
    "        M_tune.dtype = dtype_\n",
    "\n",
    "    best_overall = float(\"inf\")\n",
    "    pruner_step = 0\n",
    "\n",
    "    try:\n",
    "        # Tasks 1&2: vary phases and subsets, each phase varies over _AS_SUBSET=K graphs which can be changed \n",
    "        total_phases = _MAX_PHASES\n",
    "        As_fast = As[:min(_AS_SUBSET, len(As))]\n",
    "\n",
    "        for phase in range(total_phases):\n",
    "            # per-phase growth from the *trial* bases, then clamp to caps\n",
    "            epochs_now = int(round(epochs_     * (_GROWTH_E ** phase)))\n",
    "            m_now      = int(round(m_          * (_GROWTH_M ** phase)))\n",
    "            batch_now  = int(round(batch_size_ * (_GROWTH_B ** phase)))\n",
    "\n",
    "            epochs_now = max(1, min(_CAP_EPOCHS, epochs_now))\n",
    "            M_tune.m   = max(1, min(_CAP_M,     m_now))\n",
    "            batch_now  = max(1, min(_CAP_BATCH, batch_now))\n",
    "\n",
    "            for i, Ai in enumerate(As_fast):\n",
    "                M_tune.A = Ai\n",
    "\n",
    "                # no checkpointing during search to keep trials light\n",
    "                hist_loss, best_loss, best_epoch, _ = M_tune.train(\n",
    "                    batch_size=batch_now,\n",
    "                    grad_accu_steps=grad_accu_steps_,\n",
    "                    epochs=epochs_now,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=None,\n",
    "                    num_workers=0,\n",
    "                    checkpoint_prefix_with_path=None,\n",
    "                    progress_bar=False,\n",
    "                )\n",
    "\n",
    "                metric = float(best_loss) if len(hist_loss) else float('inf')\n",
    "                best_overall = min(best_overall, metric)\n",
    "\n",
    "                pruner_step += 1\n",
    "                trial.report(metric, pruner_step)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return best_overall\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            _free_cuda()\n",
    "            raise optuna.exceptions.TrialPruned() from e\n",
    "        raise\n",
    "    finally:\n",
    "        del net, optimizer, M_tune\n",
    "        _free_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56834a27-c291-4800-862d-9efb7060db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Study setup (safe seeding) + globals update (+schedule) \n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=10, multivariate=True, group=True)\n",
    "pruner  = MedianPruner(n_warmup_steps=2)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"gnp_hparam_search\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    "    # storage=\"sqlite:///gnp_optuna.db\", load_if_exists=True,\n",
    ")\n",
    "\n",
    "# SAFE SEED VALUES (use existing if present, else defaults) \n",
    "seed_epochs = int(min(120, int(globals().get(\"_CAP_EPOCHS\", 200))))\n",
    "seed_phases = int(globals().get(1, _MAX_PHASES))\n",
    "\n",
    "# seed with current globals + safe schedule seeds\n",
    "study.enqueue_trial({\n",
    "    \"num_layers\": num_layers,\n",
    "    \"embed\": embed,\n",
    "    \"hidden\": hidden,\n",
    "    \"drop_rate\": drop_rate,\n",
    "    \"disable_scale_input\": disable_scale_input,\n",
    "    \"dtype\": \"float64\" if dtype == torch.float64 else \"float32\",\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"training_data\": training_data,\n",
    "    \"m\": m,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"grad_accu_steps\": grad_accu_steps,\n",
    "    \"epochs\": seed_epochs,  \n",
    "    \"phases\": seed_phases,  \n",
    "})\n",
    "\n",
    "# choose amount of trials\n",
    "study.optimize(objective, n_trials=12, gc_after_trial=True, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n[Optuna] Trials: {len(study.trials)}  Best value: {study.best_value:.6e}\")\n",
    "print(\"[Optuna] Best hyperparameters:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "best = study.best_trial.params\n",
    "\n",
    "# Update core \n",
    "num_layers = int(best[\"num_layers\"])\n",
    "embed = int(best[\"embed\"])\n",
    "hidden = int(best[\"hidden\"])\n",
    "drop_rate = float(best[\"drop_rate\"])\n",
    "disable_scale_input = bool(best[\"disable_scale_input\"])\n",
    "dtype = torch.float64 if best[\"dtype\"] == \"float64\" else torch.float32\n",
    "lr = float(best[\"lr\"])\n",
    "weight_decay = float(best[\"weight_decay\"])\n",
    "training_data = str(best[\"training_data\"])\n",
    "m = int(best[\"m\"])\n",
    "batch_size = int(best[\"batch_size\"])\n",
    "grad_accu_steps = int(best[\"grad_accu_steps\"])\n",
    "\n",
    "# Export schedule knobs for the training cell\n",
    "# Use tuned 'epochs' and 'phases' if present; otherwise keep the seed values.\n",
    "PHASES = int(best.get(\"phases\", seed_phases))\n",
    "EPOCHS_PER_PHASE_BASE = int(best.get(\"epochs\", seed_epochs))\n",
    "M_BASE = int(best.get(\"m\", m))\n",
    "BATCH_BASE = int(best.get(\"batch_size\", batch_size))\n",
    "\n",
    "# Keep these consistent with Cell 1 FAST settings:\n",
    "GROWTH_EPOCHS = 1.06\n",
    "GROWTH_M = 1.06\n",
    "GROWTH_BATCH = 1.06\n",
    "MAX_EPOCHS_PER_PHASE = 40\n",
    "MAX_M = 512\n",
    "MAX_BATCH = 4\n",
    "\n",
    "# Runtime knobs used by the training cell\n",
    "CKPT_DIR = \"./checkpoints_tasks12\"\n",
    "SAVE_BEST_EACH_PASS = True\n",
    "RESUME_FROM_PHASE = 0\n",
    "\n",
    "print(\"\\n[Optuna] Globals updated (+schedule).\")\n",
    "print(f\"[Schedule] PHASES={PHASES}, EPOCHS_BASE={EPOCHS_PER_PHASE_BASE}, \"\n",
    "      f\"M_BASE={M_BASE}, BATCH_BASE={BATCH_BASE}, \"\n",
    "      f\"GROWTH(E,M,B)=({GROWTH_EPOCHS},{GROWTH_M},{GROWTH_BATCH}), \"\n",
    "      f\"MAX(E,M,B)=({MAX_EPOCHS_PER_PHASE},{MAX_M},{MAX_BATCH})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a9872-553d-4ef3-99ce-5fec08fce2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Tasks 1&2 training (uses globals set by Optuna Cell 2) \n",
    "import os, time, gc, numpy as np, torch\n",
    "\n",
    "# minimal sanity\n",
    "assert 'As' in globals() and len(As) > 0\n",
    "assert 'ResGCN' in globals() and 'GNP' in globals()\n",
    "assert 'device' in globals()\n",
    "\n",
    "# build model\n",
    "net = ResGCN(\n",
    "    As[0], num_layers, embed, hidden, drop_rate,\n",
    "    scale_input=(not disable_scale_input), dtype=dtype\n",
    ").to(device).to(dtype)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = None\n",
    "\n",
    "M = GNP(As[0], training_data, M_BASE, net, device)\n",
    "if hasattr(M, \"dtype\"): M.dtype = dtype\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for phase in range(RESUME_FROM_PHASE, PHASES):\n",
    "    # per-phase growth \n",
    "    epochs_now = int(max(1, min(MAX_EPOCHS_PER_PHASE, round(EPOCHS_PER_PHASE_BASE * (GROWTH_EPOCHS ** phase)))))\n",
    "    M.m        = int(max(1, min(MAX_M,                round(M_BASE                * (GROWTH_M      ** phase)))))\n",
    "    batch_now  = int(max(1, min(MAX_BATCH,            round(BATCH_BASE            * (GROWTH_BATCH  ** phase)))))\n",
    "\n",
    "    print(f\"\\n=== PHASE {phase+1}/{PHASES} === [epochs={epochs_now}, batch={batch_now}, m={M.m}]\")\n",
    "    tic = time.time()\n",
    "\n",
    "    for i, Ai in enumerate(As):\n",
    "        gc.collect()\n",
    "        M.A = Ai  # <-- refresh graph for this pass\n",
    "        ckpt_prefix = os.path.join(CKPT_DIR, f\"p{phase:04d}_Ai{i:03d}_\")\n",
    "\n",
    "        hist_loss, best_loss, best_epoch, model_file = M.train(\n",
    "            batch_now, grad_accu_steps, epochs_now,\n",
    "            optimizer, scheduler,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            checkpoint_prefix_with_path=(ckpt_prefix if SAVE_BEST_EACH_PASS else None),\n",
    "            progress_bar=False\n",
    "        )\n",
    "\n",
    "        # optional reload of best\n",
    "        if SAVE_BEST_EACH_PASS:\n",
    "            candidates = []\n",
    "            if model_file: candidates.append(model_file)\n",
    "            candidates += [f\"{ckpt_prefix}epoch_{best_epoch}.pt\", f\"{ckpt_prefix}best.pt\"]\n",
    "            for wpath in candidates:\n",
    "                if wpath and os.path.exists(wpath):\n",
    "                    try:\n",
    "                        state = torch.load(wpath, map_location=device, weights_only=True)\n",
    "                    except TypeError:\n",
    "                        state = torch.load(wpath, map_location=device)\n",
    "                    if isinstance(M.net, torch.nn.Module):\n",
    "                        M.net.load_state_dict(state)\n",
    "                    break\n",
    "\n",
    "        try:\n",
    "            init_loss, final_loss = float(hist_loss[0]), float(hist_loss[-1])\n",
    "        except Exception:\n",
    "            init_loss = final_loss = np.nan\n",
    "        print(f\"  Ai {i:03d} | init={init_loss:.4e}, final={final_loss:.4e}, best={float(best_loss):.4e}@{best_epoch}\")\n",
    "        del hist_loss, best_loss, best_epoch, model_file\n",
    "\n",
    "    torch.save(M.net.state_dict(), os.path.join(CKPT_DIR, f\"phase_{phase:04d}_end.pt\"))\n",
    "    print(f\"[phase {phase+1}] {time.time()-tic:.2f}s\")\n",
    "\n",
    "print(f\"\\n[Tasks 1+2] Completed in {time.time()-t0:.2f}s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec52fad-4d68-4c99-98b4-93bd0145cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Tasks 1&2 training (uses globals set by Optuna Cell 2) + logging + plots\n",
    "import os, time, gc, numpy as np, torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# minimal sanity\n",
    "assert 'As' in globals() and len(As) > 0\n",
    "assert 'ResGCN' in globals() and 'GNP' in globals()\n",
    "assert 'device' in globals()\n",
    "\n",
    "# build model\n",
    "net = ResGCN(\n",
    "    As[0], num_layers, embed, hidden, drop_rate,\n",
    "    scale_input=(not disable_scale_input), dtype=dtype\n",
    ").to(device).to(dtype)\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = None\n",
    "\n",
    "M = GNP(As[0], training_data, M_BASE, net, device)\n",
    "if hasattr(M, \"dtype\"): \n",
    "    M.dtype = dtype\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# accumulators for learning curve (mean over As) (Does not work and is not important, yet I thought I might try)\n",
    "phase_mean_init  = []\n",
    "phase_mean_final = []\n",
    "phase_mean_best  = []\n",
    "phase_best_epoch = []\n",
    "\n",
    "t0 = time.time()\n",
    "for phase in range(RESUME_FROM_PHASE, PHASES):\n",
    "    # per-phase growth \n",
    "    epochs_now = int(max(1, min(MAX_EPOCHS_PER_PHASE, round(EPOCHS_PER_PHASE_BASE * (GROWTH_EPOCHS ** phase)))))\n",
    "    M.m        = int(max(1, min(MAX_M,                round(M_BASE                * (GROWTH_M      ** phase)))))\n",
    "    batch_now  = int(max(1, min(MAX_BATCH,            round(BATCH_BASE            * (GROWTH_BATCH  ** phase)))))\n",
    "\n",
    "    print(f\"\\n=== PHASE {phase+1}/{PHASES} === [epochs={epochs_now}, batch={batch_now}, m={M.m}]\")\n",
    "    tic = time.time()\n",
    "\n",
    "    # per-phase collectors (over As)\n",
    "    inits_this_phase  = []\n",
    "    finals_this_phase = []\n",
    "    bests_this_phase  = []\n",
    "    best_epochs_this  = []\n",
    "\n",
    "    for i, Ai in enumerate(As):\n",
    "        gc.collect()\n",
    "        M.A = Ai  # refresh graph for this pass\n",
    "        ckpt_prefix = os.path.join(CKPT_DIR, f\"p{phase:04d}_Ai{i:03d}_\")\n",
    "\n",
    "        hist_loss, best_loss, best_epoch, model_file = M.train(\n",
    "            batch_now, grad_accu_steps, epochs_now,\n",
    "            optimizer, scheduler,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            checkpoint_prefix_with_path=(ckpt_prefix if SAVE_BEST_EACH_PASS else None),\n",
    "            progress_bar=False\n",
    "        )\n",
    "\n",
    "        # optional reload of best\n",
    "        if SAVE_BEST_EACH_PASS:\n",
    "            candidates = []\n",
    "            if model_file: \n",
    "                candidates.append(model_file)\n",
    "            candidates += [f\"{ckpt_prefix}epoch_{best_epoch}.pt\", f\"{ckpt_prefix}best.pt\"]\n",
    "            for wpath in candidates:\n",
    "                if wpath and os.path.exists(wpath):\n",
    "                    try:\n",
    "                        state = torch.load(wpath, map_location=device, weights_only=True)\n",
    "                    except TypeError:\n",
    "                        state = torch.load(wpath, map_location=device)\n",
    "                    if isinstance(M.net, torch.nn.Module):\n",
    "                        M.net.load_state_dict(state)\n",
    "                    break\n",
    "\n",
    "        # losses for logging\n",
    "        try:\n",
    "            init_loss, final_loss = float(hist_loss[0]), float(hist_loss[-1])\n",
    "        except Exception:\n",
    "            init_loss = final_loss = np.nan\n",
    "\n",
    "        inits_this_phase.append(init_loss)\n",
    "        finals_this_phase.append(final_loss)\n",
    "        bests_this_phase.append(float(best_loss))\n",
    "        best_epochs_this.append(int(best_epoch))\n",
    "\n",
    "        print(f\"  Ai {i:03d} | init={init_loss:.4e}, final={final_loss:.4e}, best={float(best_loss):.4e}@{best_epoch}\")\n",
    "        del hist_loss, best_loss, best_epoch, model_file\n",
    "\n",
    "    # store per-phase means (robust to NaNs)\n",
    "    phase_mean_init.append(np.nanmean(inits_this_phase)   if inits_this_phase   else np.nan)\n",
    "    phase_mean_final.append(np.nanmean(finals_this_phase) if finals_this_phase  else np.nan)\n",
    "    phase_mean_best.append(np.nanmean(bests_this_phase)   if bests_this_phase   else np.nan)\n",
    "    phase_best_epoch.append(int(np.nanmean(best_epochs_this)) if best_epochs_this else 0)\n",
    "\n",
    "    torch.save(M.net.state_dict(), os.path.join(CKPT_DIR, f\"phase_{phase:04d}_end.pt\"))\n",
    "    print(f\"[phase {phase+1}] {time.time()-tic:.2f}s\")\n",
    "\n",
    "print(f\"\\n[Tasks 1+2] Completed in {time.time()-t0:.2f}s.\")\n",
    "\n",
    "# PLOTS \n",
    "\n",
    "# Learning curve over phases (mean losses across As) \n",
    "phases_axis = np.arange(len(phase_mean_final)) + 1  # 1..PHASES\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(phases_axis, phase_mean_init,  label=\"Mean init loss per phase\")\n",
    "plt.plot(phases_axis, phase_mean_final, label=\"Mean final loss per phase\")\n",
    "plt.plot(phases_axis, phase_mean_best,  label=\"Mean best loss per phase\", linestyle=\"--\")\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "#  Iterations vs length (recompute with the trained M) \n",
    "iterations_tasks12 = np.zeros(len(As), dtype=float)\n",
    "for i, Ai in enumerate(As):\n",
    "    M.A = Ai  # ensure M uses the current graph\n",
    "    ttw = data - Ai @ data\n",
    "    # keep progress silent for clean output\n",
    "    x, iters, *_ = solver.solve(A=Ai, b=ttw, rtol=1e-6, max_iters=2000, progress_bar=True, M=M)\n",
    "    iterations_tasks12[i] = iters\n",
    "\n",
    "# x-axis: same count as As\n",
    "x_lengths = np.logspace(1, 3, len(As))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_lengths, iterations_tasks12, label=f\"{PHASES} phases (with growth)\")\n",
    "plt.xlabel(\"Length [km]\")\n",
    "plt.ylabel(\"Iterations\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74844fc7-306e-4b62-b07d-453cc4a988de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee14e89-e7b8-47a4-8fba-dc2fbaa95014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070706f-0393-4667-bfb2-5231bc84f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25765aea-9e48-4dbc-ba1c-4705fa1e47b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65827f-5190-496b-9e18-82ed3a75cbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a06f42-73c4-4064-8f73-43263704b238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4e96a-b921-4022-b24d-7345eb4432f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified config: Tasks 1 + 2 + 3 \n",
    "\n",
    "\n",
    "\n",
    "num_layers = 8              # number of layers in GNP\n",
    "embed = 32                  # embedding dimension in GNP\n",
    "hidden = 64                 # hidden dimension in MLPs in GNP\n",
    "drop_rate = 0.05            # dropout rate in GNP\n",
    "disable_scale_input = False # whether disable the scaling of inputs in GNP\n",
    "dtype = torch.float64       # training precision for GNP\n",
    "lr = 2e-3                   # learning rate in training GNP\n",
    "weight_decay = 0.0          # weight decay in training GNP\n",
    "training_data = 'x_mix'     # type of training data x\n",
    "m = 40                      # Krylov subspace dimension for training data\n",
    "batch_size = 4              # batch size in training GNP\n",
    "grad_accu_steps = 1         # gradient accumulation steps in training GNP\n",
    "epochs = 1000               # number of epochs in training GNP \n",
    "\n",
    "\n",
    "# Total number of phases (Task 2)\n",
    "PHASES = 3\n",
    "\n",
    "# Task 1: growth starting points\n",
    "EPOCHS_PER_PHASE_BASE = 2     \n",
    "M_BASE                = m     \n",
    "BATCH_BASE            = batch_size\n",
    "\n",
    "# Growth factors per phase (Task 1)\n",
    "GROWTH_EPOCHS = 1.15\n",
    "GROWTH_M      = 1.10\n",
    "GROWTH_BATCH  = 1.20\n",
    "\n",
    "# Caps (safety)\n",
    "MAX_EPOCHS_PER_PHASE = 10\n",
    "MAX_M                = M_BASE * 8\n",
    "MAX_BATCH            = BATCH_BASE * 8\n",
    "\n",
    "# Task 3: loopbacks\n",
    "LOOPBACK_EVERY = 5    # after every 5 phases, run a loopback sweep\n",
    "LOOPBACK_EPOCHS = 1   # tiny number of epochs during loopback\n",
    "\n",
    "# Courtesy to ChatGPT for Checkpointing\n",
    "CKPT_DIR = \"./ckpt_unified/\"\n",
    "SAVE_BEST_EACH_PASS = True\n",
    "RESUME_FROM_PHASE = 0\n",
    "\n",
    "print(f\"[Unified Config] PHASES={PHASES}, start epochs={EPOCHS_PER_PHASE_BASE}, m={M_BASE}, batch={BATCH_BASE}\")\n",
    "print(f\"[Growth] g_epochs={GROWTH_EPOCHS}, g_m={GROWTH_M}, g_batch={GROWTH_BATCH}\")\n",
    "print(f\"[Loopback] every {LOOPBACK_EVERY} phases, epochs/ai={LOOPBACK_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08d8ec-8793-43ad-900f-d0b9dd6cbb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified training loop: Tasks 1+2+3 (growth + many phases + loopbacks) \n",
    "import os, time, torch, numpy as np\n",
    "\n",
    "assert 'As' in globals() and isinstance(As, list) and len(As) > 0\n",
    "assert 'data' in globals()\n",
    "assert 'solver' in globals()\n",
    "\n",
    "net = ResGCN(\n",
    "    As[0], num_layers, embed, hidden, drop_rate,\n",
    "    scale_input=(not disable_scale_input), dtype=dtype\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = None\n",
    "\n",
    "M = GNP(As[0], training_data, M_BASE, net, device)\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def train_subset(A_list, epochs_per_ai, phase_tag, batch_now, m_now):\n",
    "    \"\"\"Train sequentially on a list of As with given epochs/batch/m.\"\"\"\n",
    "    M.m = m_now\n",
    "    for i, Ai in enumerate(A_list):\n",
    "        # Call garbage collector and free whatever cache there is\n",
    "        # IDK why but seems to work\n",
    "        # torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        M.A = Ai\n",
    "        ckpt_prefix = os.path.join(CKPT_DIR, f\"{phase_tag}_Ai{i:03d}_\")\n",
    "        hist_loss, best_loss, best_epoch, model_file = M.train(\n",
    "            batch_now, grad_accu_steps, epochs_per_ai,\n",
    "            optimizer, scheduler,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            checkpoint_prefix_with_path=ckpt_prefix if SAVE_BEST_EACH_PASS else None,\n",
    "            progress_bar=False\n",
    "        )\n",
    "        if SAVE_BEST_EACH_PASS:\n",
    "            candidates = []\n",
    "            if model_file: candidates.append(model_file)\n",
    "            candidates += [f\"{ckpt_prefix}epoch_{best_epoch}.pt\", f\"{ckpt_prefix}best.pt\"]\n",
    "            for wpath in candidates:\n",
    "                if os.path.exists(wpath):\n",
    "                    try:\n",
    "                        state = torch.load(wpath, map_location=device, weights_only=True)\n",
    "                    except TypeError:\n",
    "                        state = torch.load(wpath, map_location=device)\n",
    "                    M.net.load_state_dict(state)\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            init_loss = float(hist_loss[0]); final_loss = float(hist_loss[-1])\n",
    "        except Exception:\n",
    "            init_loss, final_loss = np.nan, np.nan\n",
    "        print(f\"      Ai | init={init_loss:.4e}, final={final_loss:.4e}, best={best_loss:.4e} @ epoch {best_epoch}\")\n",
    "        # Explicitly delete variables created in the loop\n",
    "        # Should happen on its own, but it doesn't\n",
    "        del hist_loss, best_loss, best_epoch, model_file\n",
    "\n",
    "# Training across all phases \n",
    "t0_all = time.time()\n",
    "for phase in range(RESUME_FROM_PHASE, PHASES):\n",
    "    # Task 1 growth (scaled with phase index)\n",
    "    epochs_now = int(max(1, min(MAX_EPOCHS_PER_PHASE, round(EPOCHS_PER_PHASE_BASE * (GROWTH_EPOCHS ** phase)))))\n",
    "    m_now      = int(max(1, min(MAX_M,                round(M_BASE                * (GROWTH_M      ** phase)))))\n",
    "    batch_now  = int(max(1, min(MAX_BATCH,            round(BATCH_BASE            * (GROWTH_BATCH  ** phase)))))\n",
    "\n",
    "    print(f\"\\n=== Phase {phase+1}/{PHASES} === [epochs/ai={epochs_now}, batch={batch_now}, m={m_now}]\")\n",
    "    tic_phase = time.time()\n",
    "    train_subset(As, epochs_now, phase_tag=f\"p{phase:04d}\", batch_now=batch_now, m_now=m_now)\n",
    "    print(f\"[phase {phase+1}] done in {time.time() - tic_phase:.2f} s\")\n",
    "\n",
    "    # Task 3 loopback: after every LOOPBACK_EVERY phases\n",
    "    if (phase + 1) % LOOPBACK_EVERY == 0:\n",
    "        print(f\"   >> Loopback after phase {phase+1} (epochs/ai={LOOPBACK_EPOCHS})\")\n",
    "        tic_lb = time.time()\n",
    "        train_subset(As, LOOPBACK_EPOCHS, phase_tag=f\"loopback_p{phase:04d}\", batch_now=batch_now, m_now=m_now)\n",
    "        print(f\"   Loopback time: {time.time() - tic_lb:.2f} s\")\n",
    "\n",
    "    torch.save(M.net.state_dict(), os.path.join(CKPT_DIR, f\"phase_{phase:04d}_end.pt\"))\n",
    "\n",
    "print(f\"\\n[Unified Training] Completed {PHASES} phases in {time.time() - t0_all:.2f} s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b1427-0be8-4b08-a346-5985545cecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_unified = np.zeros(len(As))\n",
    "for i, Ai in enumerate(As):\n",
    "    ttw = data - Ai @ data\n",
    "    x, iters, *_ = solver.solve(A=Ai, b=ttw, rtol=1e-6, max_iters=2000, progress_bar=True, M=M)\n",
    "    iterations_unified[i] = iters\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.logspace(1, 3, 10), iterations_unified, label=f\"{PHASES} phases (growth + loopbacks)\")\n",
    "\n",
    "plt.xlabel(\"Length [km]\")\n",
    "plt.ylabel(\"Iterations\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06955615-8f50-4aa1-95c3-0394ec9db97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65c96f-0b5e-480f-8351-34242823a000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9789c41-96db-4e00-87be-e4f1706fe5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2a439-948d-4c7c-9607-88b29fb9f592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762a56d-4966-49ca-b56d-979342ae3c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31645fab-b2cd-484b-a865-54ddff374d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f594c8f-8787-4784-8d83-687ab8722928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21173a36-3ce2-4d26-8951-dd73664ad8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa92018-e0b4-4b8f-bc5e-15f63e58f025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94d19e-9462-47ff-9c69-a02d5ee528c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dff5e1-f979-4521-a568-5bb26eae24d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6c01c-fcae-4898-a992-a489daf7e23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ab6e3-3d59-4096-93a0-25e5b56905a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b95b4-5306-4cbd-88e1-17aee76f77c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea4731-65df-43ed-8c3a-955674393c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3b14b-1b56-4e21-b42f-d9158a90b1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766cd1d-1114-4ed8-8e0e-89c3849df4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab5efa-271e-4c97-aefd-acfc25287cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d1c0-6f21-48a1-b8b2-c0c14e01280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Condition Number Plot ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3f47f-903b-4b84-9337-31e9976e5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scale_A_by_spectral_radius(A):\n",
    "\n",
    "    if A.layout == torch.sparse_csc:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        m, n = absA.shape\n",
    "        row_sum = absA @ torch.ones(n, 1, dtype=A.dtype, device=A.device)\n",
    "        col_sum = torch.ones(1, m, dtype=A.dtype, device=A.device) @ absA\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A * (1. / gamma.item())\n",
    "\n",
    "    elif A.layout == torch.strided:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        row_sum = torch.sum(absA, dim=1)\n",
    "        col_sum = torch.sum(absA, dim=0)\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A / gamma\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise NotImplementedError(\n",
    "            'A must be either torch.sparse_csc_tensor or torch.tensor')\n",
    "\n",
    "    return outA\n",
    "\n",
    "\n",
    "class Arnoldi():\n",
    "    def build(self, A, v0=None, m=100):\n",
    "\n",
    "        n = A.shape[0]\n",
    "        if v0 is None:\n",
    "            v0 = torch.normal(0, 1, size=(n,), dtype=A.dtype).to(A.device)\n",
    "        beta = torch.linalg.norm(v0)\n",
    "\n",
    "        V = torch.zeros(n, m + 1, dtype=A.dtype).to(A.device)\n",
    "        H = torch.zeros(m + 1, m, dtype=A.dtype).to(A.device)\n",
    "\n",
    "        V[:, 0] = v0 / beta\n",
    "        for j in range(m):\n",
    "            w = A @ V[:, j]\n",
    "            for k in range(j + 1):\n",
    "                H[k, j] = torch.dot(V[:, k], w)\n",
    "                w = w - H[k, j] * V[:, k]\n",
    "            H[j + 1, j] = torch.linalg.norm(w)\n",
    "            V[:, j + 1] = w / H[j + 1, j]\n",
    "\n",
    "        Vm1 = V\n",
    "        barHm = H\n",
    "        return Vm1, barHm\n",
    "\n",
    "\n",
    "# The following class implements a streaming dataset, which, in combined use with the dataloader, produces x of size (n,batch_size). x is float64 and stays in cpu. It will be moved to the \n",
    "# device and cast to a lower precision for training.\n",
    "class StreamingDataset(IterableDataset):\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, batch_size, training_data, m):\n",
    "        super().__init__()\n",
    "        self.n = A.shape[0]\n",
    "        self.m = m\n",
    "        self.batch_size = batch_size\n",
    "        self.training_data = training_data\n",
    "\n",
    "        # Computations done in device\n",
    "        if training_data == 'x_subspace' or training_data == 'x_mix':\n",
    "            arnoldi = Arnoldi()\n",
    "            Vm1, barHm = arnoldi.build(A, m=m)\n",
    "            W, S, Zh = torch.linalg.svd(barHm, full_matrices=False)\n",
    "            Q = (Vm1[:, :-1] @ Zh.T) / S.view(1, m)\n",
    "            self.Q = Q.to('cpu')\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "\n",
    "            # Computation done in cpu\n",
    "            if self.training_data == 'x_normal':\n",
    "\n",
    "                x = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_subspace':\n",
    "\n",
    "                e = torch.normal(0, 1, size=(self.m, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_mix':\n",
    "\n",
    "                batch_size1 = self.batch_size // 2\n",
    "                e = torch.normal(0, 1, size=(self.m, batch_size1),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                batch_size2 = self.batch_size - batch_size1\n",
    "                x2 = torch.normal(0, 1, size=(self.n, batch_size2),\n",
    "                                  dtype=torch.float64)\n",
    "                x = torch.cat([x, x2], dim=1)\n",
    "                yield x\n",
    "\n",
    "            else:  # self.training_data == 'no_x'\n",
    "\n",
    "                b = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.generate())\n",
    "\n",
    "\n",
    "class GNP():\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, training_data, m, net, device):\n",
    "        self.A = A\n",
    "        self.training_data = training_data\n",
    "        self.m = m\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.dtype = net.dtype\n",
    "\n",
    "    def train(self, batch_size, grad_accu_steps, epochs, optimizer,\n",
    "              scheduler=None, num_workers=0, checkpoint_prefix_with_path=None,\n",
    "              progress_bar=True):\n",
    "\n",
    "        self.net.train()\n",
    "        optimizer.zero_grad()\n",
    "        dataset = StreamingDataset(self.A, batch_size,\n",
    "                                   self.training_data, self.m)\n",
    "        loader = DataLoader(dataset, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        hist_loss = []\n",
    "        best_loss = np.inf\n",
    "        best_epoch = -1\n",
    "        checkpoint_file = None\n",
    "\n",
    "        if progress_bar:\n",
    "            pbar = tqdm(total=epochs, desc='Train')\n",
    "\n",
    "        for epoch, x_or_b in enumerate(loader):\n",
    "\n",
    "            # Generate training data\n",
    "            if self.training_data != 'no_x':\n",
    "                x = x_or_b[0].to(self.device)\n",
    "                b = self.A @ x\n",
    "                b, x = b.to(self.dtype), x.to(self.dtype)\n",
    "            else:  # self.training_data == 'no_x'\n",
    "                b = x_or_b[0].to(self.device).to(self.dtype)\n",
    "\n",
    "            # Train\n",
    "            x_out = self.net(b)\n",
    "            b_out = (self.A @ x_out.to(torch.float64)).to(self.dtype)\n",
    "            loss = F.l1_loss(b_out, b)\n",
    "\n",
    "            # Bookkeeping\n",
    "            hist_loss.append(loss.item())\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_epoch = epoch\n",
    "                if checkpoint_prefix_with_path is not None:\n",
    "                    checkpoint_file = checkpoint_prefix_with_path + 'best.pt'\n",
    "                    torch.save(self.net.state_dict(), checkpoint_file)\n",
    "\n",
    "            # Train (cont.)\n",
    "            loss.backward()\n",
    "            if (epoch + 1) % grad_accu_steps == 0 or epoch == epochs - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Bookkeeping (cont.)\n",
    "            if progress_bar:\n",
    "                pbar.set_description(f'Train loss {loss:.1e}')\n",
    "                pbar.update()\n",
    "            if epoch == epochs - 1:\n",
    "                break\n",
    "\n",
    "        # Bookkeeping (cont.)\n",
    "        if checkpoint_file is not None:\n",
    "            checkpoint_file_old = checkpoint_file\n",
    "            checkpoint_file = \\\n",
    "                checkpoint_prefix_with_path + f'epoch_{best_epoch}.pt'\n",
    "            os.rename(checkpoint_file_old, checkpoint_file)\n",
    "\n",
    "        return hist_loss, best_loss, best_epoch, checkpoint_file\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, r):  # r: float64\n",
    "        self.net.eval()\n",
    "        # r = r.to(self.dtype)  # -> lower precision\n",
    "        r = r.view(-1, 1)\n",
    "        z = self.net(r)\n",
    "        z = z.view(-1)\n",
    "        # z = z.double()  # -> float64\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GNP\n",
    "\n",
    "condition_numbers = []\n",
    "epochs_tracked = []\n",
    "\n",
    "dataset = StreamingDataset(A, batch_size=batch_size,\n",
    "                           training_data=training_data, m=m)\n",
    "loader = iter(torch.utils.data.DataLoader(dataset, num_workers=0, batch_size=None))\n",
    "\n",
    "log_intervals = 20  \n",
    "interval = 3000 // log_intervals\n",
    "\n",
    "print(\"Tracking condition number during training evaluation...\")\n",
    "\n",
    "for i in range(log_intervals + 1):\n",
    "    epoch = i * interval\n",
    "    x_or_b = next(loader)\n",
    "\n",
    "    if training_data != 'no_x':\n",
    "        x = x_or_b.to(device)\n",
    "        b = A @ x\n",
    "        b, x = b.to(net.dtype), x.to(net.dtype)\n",
    "    else:\n",
    "        b = x_or_b.to(device).to(net.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_out = net(b)\n",
    "\n",
    "    try:\n",
    "        ratio = torch.linalg.lstsq(x_out.double(), b.double()).solution\n",
    "        cond = np.linalg.cond(ratio.cpu().numpy())\n",
    "        condition_numbers.append(cond)\n",
    "        epochs_tracked.append(epoch)\n",
    "        print(f\"Epoch {epoch:4d}: Condition number = {cond:.2e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed at epoch {epoch}: {e}\")\n",
    "        condition_numbers.append(np.nan)\n",
    "        epochs_tracked.append(epoch)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_tracked, condition_numbers, marker='o')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Estimated Condition Number\")\n",
    "plt.title(\"Condition Number Evolution During Training\")\n",
    "plt.grid(True, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbc28e-2d14-4960-bd02-c2a81d349424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve preconditioned iterations lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dafb6-3776-4e25-842e-40e382bf9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not during training, but maybe the condition number overall for a given matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NaomiKernel)",
   "language": "python",
   "name": "naomikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
