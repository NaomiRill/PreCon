{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd84ad3-607c-4137-99dd-0de17053e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (2.28.2)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: pydantic<3 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (1.10.7)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.31.0-py2.py3-none-any.whl (355 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: pyyaml in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: packaging in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (23.1)\n",
      "Collecting typing-extensions<5,>=4.8\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from wandb) (3.2.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/spack-levante/mambaforge-22.9.0-2-Linux-x86_64-kptncg/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: typing-extensions, smmap, setproctitle, sentry-sdk, gitdb, gitpython, wandb\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/a/a270285/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed gitdb-4.0.12 gitpython-3.1.44 sentry-sdk-2.31.0 setproctitle-1.3.6 smmap-5.0.2 typing-extensions-4.14.0 wandb-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b48b62-f7ec-46cc-b324-6599bada8129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4df87-c916-4baf-a065-02a5e9b61e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.lax import fori_loop, cond, scan\n",
    "from jax import vmap, jit\n",
    "from scipy.sparse import csc_matrix, identity\n",
    "from scipy.sparse.linalg import cg, gmres\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "#Create mesh\n",
    "\n",
    "r_earth = 6400.0  # in km; wavenumbers will be in 1/km\n",
    "Lx = 1000  # km      Use 1020 and 1022 for dxm=4 and 2.\n",
    "Ly = Lx\n",
    "dxm = 5\n",
    "dym = dxm\n",
    "cyclic = 0  # 1 if mesh is cyclic\n",
    "cyclic_length = 360  # in degrees; if not cyclic, take it larger than  zonal size\n",
    "cyclic_length = cyclic_length * math.pi / 180  # DO NOT TOUCH\n",
    "meshtype = 'm'  # coordinates can be in physical measure 'm' or in radians 'r'\n",
    "cartesian = True\n",
    "\n",
    "xx = np.arange(0, Lx + 1, dxm, dtype=\"float32\")\n",
    "yy = np.arange(0, Ly + 1, dym, dtype=\"float32\")\n",
    "\n",
    "# Create uniformity in the mesh\n",
    "# xx = np.concatenate((np.arange(0, 4*Lx//20 + 1, dxm), np.arange(5*Lx//20, Lx + 1, dxm)))\n",
    "# yy = np.concatenate((np.arange(0, 4*Ly//20 + 1, dym), np.arange(5*Ly//20, Ly + 1, dym)))\n",
    "\n",
    "nx = len(xx)\n",
    "ny = len(yy)\n",
    "\n",
    "nodnum = np.arange(0, nx * ny)\n",
    "xcoord = np.tile(xx, reps=(ny, 1)).T\n",
    "ycoord = np.tile(yy, reps=(nx, 1))\n",
    "\n",
    "#Create random data with given spectra\n",
    "\n",
    "@jit\n",
    "def make_ll(x, y):\n",
    "    return jnp.sqrt(y * y + x * x)\n",
    "\n",
    "@jit\n",
    "def make_tff(ttf, ll):\n",
    "    return ttf / (jnp.power(ll, 1.5))  # 1.5 for -2 spectrum\n",
    "\n",
    "tt = 50 * (np.random.random(xcoord.shape) - 0.5)\n",
    "ttf = np.fft.fft2(tt)\n",
    "# ============\n",
    "# Make spectrum red\n",
    "# ============\n",
    "espectrum = np.zeros((nx // 2 + 1))  # Place for Fourier spectrum\n",
    "kk = np.concatenate((np.arange(0, nx // 2 + 1), np.arange(-nx // 2 + 1, 0, 1)))  # Wavenumbers\n",
    "\n",
    "ll = vmap(vmap(make_ll, in_axes=(None, 0)), in_axes=(0, None))(kk, kk)\n",
    "ttf = vmap(vmap(make_tff, in_axes=(0, 0)), in_axes=(0, 0))(ttf, ll)\n",
    "ttf = ttf.at[0, 0].set(0.0)\n",
    "\n",
    "tt = jnp.real(jnp.fft.ifft2(ttf))\n",
    "\n",
    "# == == == == == == == =\n",
    "# Reshape to 1D arrays\n",
    "# == == == == == == == =\n",
    "nodnum = np.reshape(nodnum, [ny, nx]).T\n",
    "xcoord = np.reshape(xcoord, [nx * ny])\n",
    "ycoord = np.reshape(ycoord, [nx * ny])\n",
    "tt = np.reshape(tt, [nx * ny])\n",
    "\n",
    "#Convert mesh and data to spherical geometry\n",
    "\n",
    "cartesian = False\n",
    "\n",
    "xcoord /= r_earth\n",
    "ycoord /= r_earth\n",
    "alpha = math.pi/3 + math.pi/12\n",
    "zg = np.sin(ycoord)\n",
    "xg = np.cos(ycoord) * np.cos(xcoord)\n",
    "yg = np.cos(ycoord) * np.sin(xcoord)\n",
    "# Rotate by alpha\n",
    "zn = zg * np.cos(alpha) + xg * np.sin(alpha)\n",
    "xg = -zg * np.sin(alpha) + xg * np.cos(alpha)\n",
    "# New coordinates in radians\n",
    "ycoord = np.arcsin(zn)\n",
    "xcoord = np.arctan2(yg,xg)\n",
    "# New coordinates in degrees\n",
    "ycoord = (180/math.pi) * np.arcsin(zn)\n",
    "xcoord = (180/math.pi) * np.arctan2(yg,xg)\n",
    "\n",
    "#Create Triangulation\n",
    "\n",
    "def make_tri(nodnum: jnp.ndarray, nx: int, ny: int):\n",
    "    def make_tri(n, nn):\n",
    "        return jnp.array([nodnum[nn, n], nodnum[nn+1, n], nodnum[nn, n+1]]), jnp.array([nodnum[nn+1, n], nodnum[nn+1, n+1], nodnum[nn, n+1]])\n",
    "\n",
    "    b = jnp.reshape(vmap(lambda i: jnp.tile(i, nx-1))(jnp.arange(ny-1)), [(nx-1)*(ny-1)])\n",
    "    a = jnp.tile(jnp.arange(nx-1), nx-1)\n",
    "\n",
    "    tri = vmap(make_tri)(b, a)\n",
    "    return jnp.reshape(jnp.concatenate((tri[0], tri[1]), axis=1), [2*(nx-1)*(ny-1), 3])\n",
    "\n",
    "make_tri = jit(make_tri, static_argnums=[1, 2])\n",
    "\n",
    "tri = make_tri(nodnum, nx, ny)\n",
    "tri = np.array(tri)\n",
    "n2d = len(xcoord)  # The number of vertices(nodes)\n",
    "e2d = len(tri[:, 1])  # The number of triangles(elements)\n",
    "\n",
    "tri\n",
    "\n",
    "# Create auxilliary arrays for the \n",
    "\n",
    "\n",
    "def neighboring_triangles(n2d: int, e2d: int, tri: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculate neighboring triangles for each node in a 2D mesh.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n2d : int\n",
    "        The total number of nodes in the mesh.\n",
    "\n",
    "    e2d : int\n",
    "        The total number of triangles (elements) in the mesh.\n",
    "\n",
    "    tri : np.ndarray\n",
    "        A 2D NumPy array representing the connectivity of triangles (elements) in the mesh.\n",
    "        Each row contains the indices of the three nodes that form a triangle.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    ne_num : np.ndarray\n",
    "        A 1D NumPy array of shape (n2d,) containing the count of neighboring triangles for each node.\n",
    "\n",
    "    ne_pos : np.ndarray\n",
    "        A 2D NumPy array of shape (max_neighboring_triangles, n2d) containing the positions of neighboring\n",
    "        triangles for each node. The 'ne_pos[i, j]' entry indicates the index of the i-th neighboring triangle\n",
    "        for the j-th node. Unused entries are filled with zeros.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the count of neighboring triangles for each node.\n",
    "    ne_num = np.zeros([n2d], dtype=int)\n",
    "\n",
    "    # Loop through each triangle (element) in the mesh.\n",
    "    for n in range(e2d):\n",
    "        enodes = tri[n, :]\n",
    "        # Increment the count of neighboring triangles for each node in the current triangle.\n",
    "        ne_num[enodes] += 1\n",
    "\n",
    "    # Initialize an array to store the positions of neighboring triangles for each node.\n",
    "    ne_pos = np.zeros([int(np.max(ne_num)), n2d], dtype=int)\n",
    "\n",
    "    # Reset the array to store the count of neighboring triangles for each node.\n",
    "    ne_num = np.zeros([n2d], dtype=int)\n",
    "\n",
    "    # Loop through each triangle (element) in the mesh.\n",
    "    for n in range(e2d):\n",
    "        enodes = tri[n, :]\n",
    "        # Loop through the nodes of the current triangle.\n",
    "        for j in range(3):\n",
    "            # Store the position of the current neighboring triangle for the corresponding node.\n",
    "            ne_pos[ne_num[enodes[j]], enodes[j]] = n\n",
    "            # Increment the count of neighboring triangles for the node.\n",
    "        ne_num[enodes] += 1\n",
    "\n",
    "    return ne_num, ne_pos\n",
    "\n",
    "def neighbouring_nodes(n2d: int, tri: np.ndarray, ne_num: np.ndarray, ne_pos: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute neighboring nodes for each node in a 2D mesh.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n2d : int\n",
    "        The total number of nodes in the mesh.\n",
    "\n",
    "    tri : np.ndarray\n",
    "        A 2D NumPy array representing the connectivity of triangles (elements) in the mesh.\n",
    "        Each row contains the indices of the three nodes that form a triangle.\n",
    "\n",
    "    ne_num : np.ndarray\n",
    "        A 1D NumPy array of shape (n2d,) containing the count of neighboring triangles for each node.\n",
    "\n",
    "    ne_pos : np.ndarray\n",
    "        A 2D NumPy array of shape (max_neighboring_triangles, n2d) containing the positions of neighboring\n",
    "        triangles for each node. The 'ne_pos[i, j]' entry indicates the index of the i-th neighboring triangle\n",
    "        for the j-th node. Unused entries are filled with zeros.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    nn_num : np.ndarray\n",
    "        A 1D NumPy array of shape (n2d,) containing the number of neighboring nodes for each node.\n",
    "\n",
    "    nn_pos : np.ndarray\n",
    "        A 2D NumPy array of shape (max_neighboring_nodes, n2d) containing the positions of neighboring nodes\n",
    "        for each node. The 'nn_pos[i, j]' entry indicates the index of the i-th neighboring node for the j-th node.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store the count of neighboring nodes for each node.\n",
    "\n",
    "\n",
    "    # Initialize an array to store the positions of neighboring nodes for each node.\n",
    "    nn_num = np.zeros([n2d], dtype=int)\n",
    "    check = np.zeros([n2d], dtype=int)\n",
    "    aux = np.zeros([10], dtype=int)\n",
    "    for j in range(n2d):\n",
    "        cc = 0\n",
    "        for m in range(ne_num[j]):\n",
    "            el = ne_pos[m, j]\n",
    "            for k in range(3):\n",
    "                a = tri[el, k]\n",
    "                if check[a] == 0:\n",
    "                    check[a] = 1\n",
    "                    aux[cc] = a\n",
    "                    cc += 1\n",
    "\n",
    "        nn_num[j] = cc\n",
    "        check[aux[0:cc]] = 0\n",
    "\n",
    "    nn_pos = np.zeros([np.max(nn_num), n2d], dtype=int)\n",
    "\n",
    "    for j in range(n2d):\n",
    "        cc = 0\n",
    "        for m in range(ne_num[j]):\n",
    "            el = ne_pos[m, j]\n",
    "            for k in range(3):\n",
    "                a = tri[el, k]\n",
    "                if check[a] == 0:\n",
    "                    check[a] = 1\n",
    "                    aux[cc] = a\n",
    "                    cc += 1\n",
    "\n",
    "        nn_pos[0:cc, j] = aux[0:cc].T\n",
    "        check[aux[0:cc]] = 0\n",
    "\n",
    "    return nn_num, nn_pos\n",
    "\n",
    "def areas(n2d: int, e2d: int, tri: np.ndarray, xcoord: np.ndarray, ycoord: np.ndarray, ne_num: np.ndarray,\n",
    "          ne_pos: np.ndarray, meshtype: str, carthesian: bool, cyclic_length):\n",
    "    \"\"\"\n",
    "    Calculate areas of triangles and derivatives of P1 basis functions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    n2d : int\n",
    "        The total number of nodes in the mesh.\n",
    "\n",
    "    e2d : int\n",
    "        The total number of triangles (elements) in the mesh.\n",
    "\n",
    "    tri : np.ndarray\n",
    "        A 2D NumPy array representing the connectivity of triangles (elements) in the mesh.\n",
    "        Each row contains the indices of the three nodes that form a triangle.\n",
    "\n",
    "    xcoord : np.ndarray\n",
    "        A 1D NumPy array containing the x-coordinates of nodes in the mesh.\n",
    "\n",
    "    ycoord : np.ndarray\n",
    "        A 1D NumPy array containing the y-coordinates of nodes in the mesh.\n",
    "\n",
    "    ne_num : np.ndarray\n",
    "        A 1D NumPy array of shape (n2d,) containing the count of neighboring triangles for each node.\n",
    "\n",
    "    ne_pos : np.ndarray\n",
    "        A 2D NumPy array of shape (max_neighboring_triangles, n2d) containing the positions of neighboring\n",
    "        triangles for each node. The 'ne_pos[i, j]' entry indicates the index of the i-th neighboring triangle\n",
    "        for the j-th node. Unused entries are filled with zeros.\n",
    "\n",
    "    meshtype : str\n",
    "        Mesh type, either 'm' (metric) or 'r' (radial).\n",
    "\n",
    "    carthesian : bool\n",
    "        Boolean indicating whether the mesh is in Cartesian coordinates.\n",
    "\n",
    "    cyclic_length : float\n",
    "        The length of the cyclic boundary if the mesh is cyclic (for 'r' meshtype).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    area : np.ndarray\n",
    "        A 1D NumPy array of shape (n2d,) containing the scalar cell (cluster) area for each node.\n",
    "\n",
    "    elem_area : np.ndarray\n",
    "        A 1D NumPy array of shape (e2d,) containing the area of each triangle (element) in the mesh.\n",
    "\n",
    "    dx : np.ndarray\n",
    "        A 2D NumPy array of shape (e2d, 3) containing the x-derivative of P1 basis functions for each triangle.\n",
    "\n",
    "    dy : np.ndarray\n",
    "        A 2D NumPy array of shape (e2d, 3) containing the y-derivative of P1 basis functions for each triangle.\n",
    "\n",
    "    Mt : np.ndarray\n",
    "        A 1D NumPy array of shape (e2d,) containing a factor for metric terms based on meshtype and coordinates.\n",
    "    \"\"\"\n",
    "    dx = np.zeros([e2d, 3], dtype=float)\n",
    "    dy = np.zeros([e2d, 3], dtype=float)\n",
    "    elem_area = np.zeros([e2d])\n",
    "    r_earth = 6400  # Earth's radius, assuming units in kilometers\n",
    "    Mt = np.ones([e2d])\n",
    "\n",
    "    if meshtype == 'm':\n",
    "        for n in range(e2d):\n",
    "            # Calculate differences in x and y coordinates for triangle vertices.\n",
    "            x2 = xcoord[tri[n, 1]] - xcoord[tri[n, 0]]\n",
    "            x3 = xcoord[tri[n, 2]] - xcoord[tri[n, 0]]\n",
    "            y2 = ycoord[tri[n, 1]] - ycoord[tri[n, 0]]\n",
    "            y3 = ycoord[tri[n, 2]] - ycoord[tri[n, 0]]\n",
    "\n",
    "            # Calculate determinant of the Jacobian matrix for this triangle.\n",
    "            d = x2 * y3 - y2 * x3\n",
    "\n",
    "            # Calculate x and y derivatives of P1 basis functions.\n",
    "            dx[n, 0] = (-y3 + y2) / d\n",
    "            dx[n, 1] = y3 / d\n",
    "            dx[n, 2] = -y2 / d\n",
    "\n",
    "            dy[n, 0] = -(-x3 + x2) / d\n",
    "            dy[n, 1] = -x3 / d\n",
    "            dy[n, 2] = x2 / d\n",
    "\n",
    "            # Calculate the area of the triangle.\n",
    "            elem_area[n] = 0.5 * abs(d)\n",
    "\n",
    "    elif meshtype == 'r':\n",
    "        rad = math.pi / 180.0\n",
    "        if carthesian:\n",
    "            Mt = np.ones([e2d])\n",
    "        else:\n",
    "            Mt = np.cos(np.sum(rad * ycoord[tri], axis=1) / 3.0)\n",
    "\n",
    "        for n in range(e2d):\n",
    "            # Calculate differences in longitude and latitude for triangle vertices.\n",
    "            x2 = rad * (xcoord[tri[n, 1]] - xcoord[tri[n, 0]])\n",
    "            x3 = rad * (xcoord[tri[n, 2]] - xcoord[tri[n, 0]])\n",
    "            y2 = r_earth * rad * (ycoord[tri[n, 1]] - ycoord[tri[n, 0]])\n",
    "            y3 = r_earth * rad * (ycoord[tri[n, 2]] - ycoord[tri[n, 0]])\n",
    "\n",
    "            # Adjust for cyclic boundaries.\n",
    "            if x2 > cyclic_length / 2.0:\n",
    "                x2 = x2 - cyclic_length\n",
    "            if x2 < -cyclic_length / 2.0:\n",
    "                x2 = x2 + cyclic_length\n",
    "            if x3 > cyclic_length / 2.0:\n",
    "                x3 = x3 - cyclic_length\n",
    "            if x3 < -cyclic_length / 2.0:\n",
    "                x3 = x3 + cyclic_length\n",
    "\n",
    "            # Apply metric factors and calculate x and y derivatives of P1 basis functions.\n",
    "            x2 = r_earth * x2 * Mt[n]\n",
    "            x3 = r_earth * x3 * Mt[n]\n",
    "            d = x2 * y3 - y2 * x3\n",
    "\n",
    "            dx[n, 0] = (-y3 + y2) / d\n",
    "            dx[n, 1] = y3 / d\n",
    "            dx[n, 2] = -y2 / d\n",
    "\n",
    "            dy[n, 0] = -(-x3 + x2) / d\n",
    "            dy[n, 1] = -x3 / d\n",
    "            dy[n, 2] = x2 / d\n",
    "\n",
    "            # Calculate the area of the triangle.\n",
    "            elem_area[n] = 0.5 * abs(d)\n",
    "\n",
    "        if carthesian:\n",
    "            Mt = np.zeros([e2d])\n",
    "        else:\n",
    "            Mt = (np.sin(rad * np.sum(ycoord[tri], axis=1) / 3.0) / Mt) / r_earth\n",
    "\n",
    "    # Calculate scalar cell (cluster) area for each node.\n",
    "    area = np.zeros([n2d])\n",
    "    for n in range(n2d):\n",
    "        area[n] = np.sum(elem_area[ne_pos[0:ne_num[n], n]]) / 3.0\n",
    "\n",
    "    return area, elem_area, dx, dy, Mt\n",
    "\n",
    "ne_num, ne_pos = neighboring_triangles(n2d, e2d, tri)\n",
    "nn_num, nn_pos = neighbouring_nodes(n2d, tri, ne_num, ne_pos)\n",
    "area, elem_area, dx, dy, Mt = areas(n2d, e2d, tri, xcoord, ycoord, ne_num, ne_pos, meshtype, cartesian, cyclic_length)\n",
    "\n",
    "Kc = np.array([2, 2.2, 2.5, 3, 4, 5, 6, 8, 10, 12, 16, 20, 24, 30, 40, 60, 80, 100]) * dxm\n",
    "Kc = 2 * math.pi / Kc  # Wavenumbers\n",
    "nr = len(Kc)\n",
    "\n",
    "# Move data to JAX\n",
    "jKc = jnp.array(Kc)\n",
    "jescaling = jnp.zeros([nr + 1])  # Place for spectrum\n",
    "# # Compute total energy\n",
    "jescaling.at[0].set(jnp.sum(area * jnp.square(tt)) / jnp.sum(area))\n",
    "jelem_area = jnp.array(elem_area)\n",
    "jdx = jnp.array(dx)\n",
    "jdy = jnp.array(dy)\n",
    "jnn_num = jnp.array(nn_num)\n",
    "jnn_pos = jnp.array(nn_pos)\n",
    "jtri = jnp.array(tri)\n",
    "jarea = jnp.array(area)\n",
    "jMt = jnp.array(Mt)\n",
    "jtt = jnp.array(tt)\n",
    "\n",
    "# Create Laplacian matrix\n",
    "\n",
    "def make_smooth(Mt: jnp.ndarray, elem_area: jnp.ndarray, dx: jnp.ndarray, dy: jnp.ndarray, nn_num: jnp.ndarray,\n",
    "                nn_pos: jnp.ndarray, tri: jnp.ndarray, n2d: int, e2d: int, full: bool = False):\n",
    "    \"\"\"\n",
    "    Calculate the smoothness matrix and metric matrix for a given mesh.\n",
    "\n",
    "    WARNING!!! This function is inherently single-threaded!. Running it on GPU will cause performance issue\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    Mt : jnp.ndarray\n",
    "        A 1D JAX NumPy array containing metric factors for each triangle.\n",
    "\n",
    "    elem_area : jnp.ndarray\n",
    "        A 1D JAX NumPy array containing the area of each triangle.\n",
    "\n",
    "    dx : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (e2d, 3) containing x-derivatives of P1 basis functions.\n",
    "\n",
    "    dy : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (e2d, 3) containing y-derivatives of P1 basis functions.\n",
    "\n",
    "    nn_num : jnp.ndarray\n",
    "        A 1D JAX NumPy array of shape (n2d,) containing the number of neighboring nodes for each node.\n",
    "\n",
    "    nn_pos : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (max_neighboring_nodes, n2d) containing positions of neighboring nodes.\n",
    "\n",
    "    tri : jnp.ndarray\n",
    "        A 2D JAX NumPy array representing the connectivity of triangles in the mesh.\n",
    "\n",
    "    n2d : int\n",
    "        The total number of nodes in the mesh.\n",
    "\n",
    "    e2d : int\n",
    "        The total number of triangles (elements) in the mesh.\n",
    "\n",
    "    full : bool, optional\n",
    "        A flag indicating whether to use the 'full' calculation including metric factors (True) or not (False).\n",
    "        Default is True.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    smooth_m : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (max_neighboring_nodes, n2d) containing the smoothness matrix.\n",
    "\n",
    "    metric : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (max_neighboring_nodes, n2d) containing the metric matrix.\n",
    "    \"\"\"\n",
    "    smooth_m = jnp.zeros(nn_pos.shape, dtype=jnp.float32)\n",
    "    metric = jnp.zeros(nn_pos.shape, dtype=jnp.float32)\n",
    "    aux = jnp.zeros((n2d,), dtype=jnp.int32)\n",
    "\n",
    "    @jit\n",
    "    def loop_body(j, carry):\n",
    "        smooth_m, metric, aux, nn_num, nn_pos, elem_area, dx, dy, Mt = carry\n",
    "        enodes = tri[j, :]\n",
    "\n",
    "        def inner_loop_body(n, carry):\n",
    "            smooth_m, metric, aux, enodes, nn_num, nn_pos, elem_area, dx, dy, Mt = carry\n",
    "            row = enodes[n]\n",
    "            cc = nn_num[row]\n",
    "\n",
    "            def fill_xd(i, val):\n",
    "                row, aux, nn_pos = val\n",
    "                n = nn_pos[i, row]\n",
    "                aux = aux.at[n].set(i)\n",
    "                return row, aux, nn_pos\n",
    "\n",
    "            row, aux, _ = fori_loop(0, cc, fill_xd, (row, aux, nn_pos))\n",
    "\n",
    "            def update_smooth_m(m, carry):\n",
    "                smooth_m, metric, aux, enodes, elem_area, dx, dy, n = carry\n",
    "                col = enodes[m]\n",
    "                pos = aux[col]\n",
    "                tmp_x = dx[m] * dx[n]\n",
    "                tmp_y = dy[n] * dy[m]\n",
    "                c1 = m == n\n",
    "\n",
    "                smooth_m = smooth_m.at[pos, row].add(cond(c1 & full,\n",
    "                                                          lambda: (tmp_x + tmp_y) * elem_area + jnp.square(\n",
    "                                                              Mt) * elem_area / 3.0,\n",
    "                                                          lambda: (tmp_x + tmp_y) * elem_area\n",
    "                                                          )\n",
    "                                                     )\n",
    "                metric = metric.at[pos, row].add(Mt * (dx[n] - dx[m]) * elem_area / 3.0)\n",
    "                return smooth_m, metric, aux, enodes, elem_area, dx, dy, n\n",
    "\n",
    "            smooth_m, metric, aux, _, _, _, _, _ = fori_loop(0, 3, update_smooth_m,\n",
    "                                                             (smooth_m, metric, aux, enodes, elem_area, dx, dy, n))\n",
    "            return smooth_m, metric, aux, enodes, nn_num, nn_pos, elem_area, dx, dy, Mt\n",
    "\n",
    "        smooth_m, metric, aux, _, _, _, _, _, _, _ = fori_loop(0, 3, inner_loop_body, (\n",
    "        smooth_m, metric, aux, enodes, nn_num, nn_pos, elem_area[j], dx[j, :], dy[j, :], Mt[j]))\n",
    "        return smooth_m, metric, aux, nn_num, nn_pos, elem_area, dx, dy, Mt\n",
    "\n",
    "    smooth_m, metric, _, _, _, _, _, _, _ = fori_loop(0, e2d, loop_body,\n",
    "                                                      (smooth_m, metric, aux, nn_num, nn_pos, elem_area, dx, dy, Mt))\n",
    "    return smooth_m, metric\n",
    "\n",
    "@partial(jit, static_argnums=[3, 4])\n",
    "def make_smat(nn_pos: jnp.ndarray, nn_num: jnp.ndarray, smooth_m: jnp.ndarray, n2d: int, nza: int):\n",
    "    \"\"\"\n",
    "    Convert the smoothness matrix into a redundant sparse form (s(k), i(k), j(k)) as required by scipy.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    nn_pos : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (max_neighboring_nodes, n2d) containing positions of neighboring nodes.\n",
    "\n",
    "    nn_num : jnp.ndarray\n",
    "        A 1D JAX NumPy array of shape (n2d,) containing the number of neighboring nodes for each node.\n",
    "\n",
    "    smooth_m : jnp.ndarray\n",
    "        A 2D JAX NumPy array of shape (max_neighboring_nodes, n2d) containing the smoothness matrix.\n",
    "\n",
    "    n2d : int\n",
    "        The total number of nodes in the mesh.\n",
    "\n",
    "    nza : int\n",
    "        The total number of nonzero elements.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    ss : jnp.ndarray\n",
    "        A 1D JAX NumPy array of shape (nza,) containing the nonzero entries of the sparse matrix.\n",
    "\n",
    "    ii : jnp.ndarray\n",
    "        A 1D JAX NumPy array of shape (nza,) containing the row indices of the nonzero entries.\n",
    "\n",
    "    jj : jnp.ndarray\n",
    "        A 1D JAX NumPy array of shape (nza,) containing the column indices of the nonzero entries.\n",
    "    \"\"\"\n",
    "\n",
    "    def helper(carry, x):\n",
    "        n, m = carry\n",
    "        out = (smooth_m[m, n], n, nn_pos[m, n])\n",
    "        n, m = cond(m + 1 >= nn_num[n], lambda: cond(n + 1 >= n2d, lambda: (0, 0), lambda: (n + 1, 0)),\n",
    "                    lambda: (n, m + 1))\n",
    "        return (n, m), out\n",
    "\n",
    "    _, tmp = scan(helper, init=(0, 0), xs=jnp.arange(nza))\n",
    "    ss, ii, jj = tmp\n",
    "\n",
    "    return ss, ii, jj\n",
    "\n",
    "with jax.default_device(jax.devices(\"cpu\")[0]): # Force JAX to use CPU\n",
    "    jsmooth, metric = make_smooth(jMt, jelem_area, jdx, jdy, jnn_num, jnn_pos, jtri, n2d, e2d, False)\n",
    "    ss, ii, jj = make_smat(jnn_pos, jnn_num, jsmooth, n2d, int(jnp.sum(jnn_num)))\n",
    "\n",
    "# Maybe Here?\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Arnoldi():\n",
    "    def build(self, A, v0=None, m=100):\n",
    "\n",
    "        n = A.shape[0]\n",
    "        if v0 is None:\n",
    "            v0 = torch.normal(0, 1, size=(n,), dtype=A.dtype).to(A.device)\n",
    "        beta = torch.linalg.norm(v0)\n",
    "\n",
    "        V = torch.zeros(n, m + 1, dtype=A.dtype).to(A.device)\n",
    "        H = torch.zeros(m + 1, m, dtype=A.dtype).to(A.device)\n",
    "\n",
    "        V[:, 0] = v0 / beta\n",
    "        for j in range(m):\n",
    "            w = A @ V[:, j]\n",
    "            for k in range(j + 1):\n",
    "                H[k, j] = torch.dot(V[:, k], w)\n",
    "                w = w - H[k, j] * V[:, k]\n",
    "            H[j + 1, j] = torch.linalg.norm(w)\n",
    "            V[:, j + 1] = w / H[j + 1, j]\n",
    "\n",
    "        Vm1 = V\n",
    "        barHm = H\n",
    "        return Vm1, barHm\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# The following class implements a streaming dataset, which, in\n",
    "# combined use with the dataloader, produces x of size (n,\n",
    "# batch_size). x is float64 and stays in cpu. It will be moved to the\n",
    "# device and cast to a lower precision for training.\n",
    "class StreamingDataset(IterableDataset):\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, batch_size, training_data, m):\n",
    "        super().__init__()\n",
    "        self.n = A.shape[0]\n",
    "        self.m = m\n",
    "        self.batch_size = batch_size\n",
    "        self.training_data = training_data\n",
    "\n",
    "        # Computations done in device\n",
    "        if training_data == 'x_subspace' or training_data == 'x_mix':\n",
    "            arnoldi = Arnoldi()\n",
    "            Vm1, barHm = arnoldi.build(A, m=m)\n",
    "            W, S, Zh = torch.linalg.svd(barHm, full_matrices=False)\n",
    "            Q = (Vm1[:, :-1] @ Zh.T) / S.view(1, m)\n",
    "            self.Q = Q.to('cpu')\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "\n",
    "            # Computation done in cpu\n",
    "            if self.training_data == 'x_normal':\n",
    "\n",
    "                x = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_subspace':\n",
    "\n",
    "                e = torch.normal(0, 1, size=(self.m, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                yield x\n",
    "\n",
    "            elif self.training_data == 'x_mix':\n",
    "\n",
    "                batch_size1 = self.batch_size // 2\n",
    "                e = torch.normal(0, 1, size=(self.m, batch_size1),\n",
    "                                 dtype=torch.float64)\n",
    "                x = self.Q @ e\n",
    "                batch_size2 = self.batch_size - batch_size1\n",
    "                x2 = torch.normal(0, 1, size=(self.n, batch_size2),\n",
    "                                  dtype=torch.float64)\n",
    "                x = torch.cat([x, x2], dim=1)\n",
    "                yield x\n",
    "\n",
    "            else:  # self.training_data == 'no_x'\n",
    "\n",
    "                b = torch.normal(0, 1, size=(self.n, self.batch_size),\n",
    "                                 dtype=torch.float64)\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.generate())\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Graph neural preconditioner\n",
    "class GNP():\n",
    "\n",
    "    # A is torch tensor, either sparse or full\n",
    "    def __init__(self, A, training_data, m, net, device):\n",
    "        self.A = A\n",
    "        self.training_data = training_data\n",
    "        self.m = m\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.dtype = net.dtype\n",
    "\n",
    "    def train(self, batch_size, grad_accu_steps, epochs, optimizer,\n",
    "              scheduler=None, num_workers=0, checkpoint_prefix_with_path=None,\n",
    "              progress_bar=True):\n",
    "\n",
    "        self.net.train()\n",
    "        optimizer.zero_grad()\n",
    "        dataset = StreamingDataset(self.A, batch_size,\n",
    "                                   self.training_data, self.m)\n",
    "        loader = DataLoader(dataset, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        hist_loss = []\n",
    "        best_loss = np.inf\n",
    "        best_epoch = -1\n",
    "        checkpoint_file = None\n",
    "\n",
    "        if progress_bar:\n",
    "            pbar = tqdm(total=epochs, desc='Train')\n",
    "\n",
    "        for epoch, x_or_b in enumerate(loader):\n",
    "\n",
    "            # Generate training data\n",
    "            if self.training_data != 'no_x':\n",
    "                x = x_or_b[0].to(self.device)\n",
    "                b = self.A @ x\n",
    "                b, x = b.to(self.dtype), x.to(self.dtype)\n",
    "            else:  # self.training_data == 'no_x'\n",
    "                b = x_or_b[0].to(self.device).to(self.dtype)\n",
    "\n",
    "            # Train\n",
    "            x_out = self.net(b)\n",
    "            b_out = (self.A @ x_out.to(torch.float64)).to(self.dtype)\n",
    "            loss = F.l1_loss(b_out, b)\n",
    "\n",
    "            wandb.log({\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"grad_norm\": torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0).item()\n",
    "            })\n",
    "\n",
    "\n",
    "            # Bookkeeping\n",
    "            hist_loss.append(loss.item())\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_epoch = epoch\n",
    "                if checkpoint_prefix_with_path is not None:\n",
    "                    checkpoint_file = checkpoint_prefix_with_path + 'best.pt'\n",
    "                    torch.save(self.net.state_dict(), checkpoint_file)\n",
    "\n",
    "            # Train (cont.)\n",
    "            loss.backward()\n",
    "            if (epoch + 1) % grad_accu_steps == 0 or epoch == epochs - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Bookkeeping (cont.)\n",
    "            if progress_bar:\n",
    "                pbar.set_description(f'Train loss {loss:.1e}') #Both works for progress bars sometimes\n",
    "                #pbar.set_postfix(loss=f\"{loss.item():.2e}\")   #This fixes it to one bar instead of each individual\n",
    "                pbar.update()\n",
    "            if epoch == epochs - 1:\n",
    "                break\n",
    "\n",
    "        # Bookkeeping (cont.)\n",
    "        if checkpoint_file is not None:\n",
    "            checkpoint_file_old = checkpoint_file\n",
    "            checkpoint_file = \\\n",
    "                checkpoint_prefix_with_path + f'epoch_{best_epoch}.pt'\n",
    "            os.rename(checkpoint_file_old, checkpoint_file)\n",
    "\n",
    "        return hist_loss, best_loss, best_epoch, checkpoint_file\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, r):  # r: float64\n",
    "        self.net.eval()\n",
    "        # r = r.to(self.dtype)  # -> lower precision\n",
    "        r = r.view(-1, 1)\n",
    "        z = self.net(r)\n",
    "        z = z.view(-1)\n",
    "        # z = z.double()  # -> float64\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "# Convention: M \\approx inv(A)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# (Right-)Preconditioned generalized minimal residual method. Can use\n",
    "# flexible preconditioner. Current implementation supports only a\n",
    "# single right-hand side.\n",
    "#\n",
    "# If timeout is not None, max_iters is disabled.\n",
    "class GMRES():\n",
    "\n",
    "    def solve(self, A, b, M=None, x0=None, restart=10, max_iters=100,\n",
    "              timeout=None, rtol=1e-8, progress_bar=True):\n",
    "\n",
    "        if progress_bar:\n",
    "            if timeout is None:\n",
    "                pbar = tqdm(total=max_iters, desc='Solve')\n",
    "                pbar.update()\n",
    "            else:\n",
    "                pbar = tqdm(desc='Solve')\n",
    "                pbar.update()\n",
    "                        \n",
    "        if x0 is None:\n",
    "            x = torch.zeros_like(b)\n",
    "        else:\n",
    "            x = x0\n",
    "        norm_b = torch.linalg.norm(b)\n",
    "        hist_abs_res = []\n",
    "        hist_rel_res = []\n",
    "        hist_time = []\n",
    "\n",
    "        n = len(b)\n",
    "        V = torch.zeros(n, restart+1, dtype=b.dtype).to(b.device)\n",
    "        Z = torch.zeros(n, restart, dtype=b.dtype).to(b.device)\n",
    "        H = torch.zeros(restart+1, restart, dtype=b.dtype).to(b.device)\n",
    "        g = torch.zeros(restart+1, dtype=b.dtype).to(b.device)\n",
    "        c = torch.zeros(restart, dtype=b.dtype).to(b.device)\n",
    "        s = torch.zeros(restart, dtype=b.dtype).to(b.device)\n",
    "        \n",
    "        tic = time.time()\n",
    "\n",
    "        # Initial step\n",
    "        r = b - A @ x\n",
    "        beta = torch.linalg.norm(r)\n",
    "        abs_res = beta\n",
    "        rel_res = abs_res / norm_b\n",
    "        hist_abs_res.append(abs_res.item())\n",
    "        hist_rel_res.append(rel_res.item())\n",
    "        hist_time.append(time.time() - tic)\n",
    "        iters = 0\n",
    "        quit_iter = False\n",
    "\n",
    "        # Outer loop\n",
    "        while 1:\n",
    "\n",
    "            # Restart cycle\n",
    "            V[:,0] = r / beta\n",
    "            g[0] = beta\n",
    "            for j in range(restart):\n",
    "                if M is not None:\n",
    "                    Z[:,j] = M.apply(V[:,j])\n",
    "                else:\n",
    "                    Z[:,j] = V[:,j]\n",
    "                w = A @ Z[:,j]\n",
    "                for k in range(j+1):\n",
    "                    H[k,j] = torch.dot(V[:,k], w)\n",
    "                    w = w - H[k,j] * V[:,k]\n",
    "                H[j+1,j] = torch.linalg.norm(w)\n",
    "                V[:,j+1] = w / H[j+1,j]\n",
    "\n",
    "                # Solve min || H * y - beta * e1 ||: Givens rotation\n",
    "                for k in range(j):\n",
    "                    tmp      =  c[k] * H[k,j] + s[k] * H[k+1,j]\n",
    "                    H[k+1,j] = -s[k] * H[k,j] + c[k] * H[k+1,j]\n",
    "                    H[k,j] = tmp\n",
    "                t = torch.sqrt( H[j,j]**2 + H[j+1,j]**2 )\n",
    "                c[j], s[j] = H[j,j]/t, H[j+1,j]/t\n",
    "                H[j,j] = c[j] * H[j,j] + s[j] * H[j+1,j]\n",
    "                H[j+1,j] = 0\n",
    "                g[j+1] = -s[j] * g[j]\n",
    "                g[j] = c[j] * g[j]\n",
    "                # End solve min || H * y - beta * e1 ||: Givens rotation\n",
    "\n",
    "                abs_res = torch.abs(g[j+1])\n",
    "                rel_res = abs_res / norm_b\n",
    "                hist_abs_res.append(abs_res.item())\n",
    "                hist_rel_res.append(rel_res.item())\n",
    "                hist_time.append(time.time() - tic)\n",
    "                iters = iters + 1\n",
    "                if (rel_res < rtol) or \\\n",
    "                   (timeout is None and iters == max_iters) or \\\n",
    "                   (timeout is not None and hist_time[-1] >= timeout):\n",
    "                    quit_iter = True\n",
    "                    break\n",
    "\n",
    "                if progress_bar:\n",
    "                    pbar.update()\n",
    "            # End restart cycle\n",
    "\n",
    "            # Solve min || H * y - beta * e1 ||: obtain solution\n",
    "            y = torch.linalg.solve_triangular(H[:j+1, :j+1],\n",
    "                                              g[:j+1].view(j+1,1),\n",
    "                                              upper=True).view(j+1)\n",
    "            # End solve min || H * y - beta * e1 ||: obtain solution\n",
    "\n",
    "            x = x + Z[:, :j+1] @ y\n",
    "            r = b - A @ x\n",
    "            beta = torch.linalg.norm(r)\n",
    "            if np.allclose(hist_abs_res[-1], beta.item()) is False:\n",
    "                warn('Residual tracked by least squares solve is different '\n",
    "                     'from the true residual. The result of GMRES should not '\n",
    "                     'be trusted.')\n",
    "            if quit_iter == True:\n",
    "                break\n",
    "            else:\n",
    "                g = g.zero_()\n",
    "                g[0] = beta\n",
    "        # End outer loop\n",
    "\n",
    "        if progress_bar:\n",
    "            pbar.close()\n",
    "\n",
    "        return x, iters, hist_abs_res, hist_rel_res, hist_time\n",
    "            \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Arnoldi process (m steps).\n",
    "class Arnoldi():\n",
    "\n",
    "    def build(self, A, v0=None, m=100):\n",
    "\n",
    "        n = A.shape[0]\n",
    "        if v0 is None:\n",
    "            v0 = torch.normal(0, 1, size=(n,), dtype=A.dtype).to(A.device)\n",
    "        beta = torch.linalg.norm(v0)\n",
    "        \n",
    "        V = torch.zeros(n, m+1, dtype=A.dtype).to(A.device)\n",
    "        H = torch.zeros(m+1, m, dtype=A.dtype).to(A.device)\n",
    "\n",
    "        V[:,0] = v0 / beta\n",
    "        for j in range(m):\n",
    "            w = A @ V[:,j]\n",
    "            for k in range(j+1):\n",
    "                H[k,j] = torch.dot(V[:,k], w)\n",
    "                w = w - H[k,j] * V[:,k]\n",
    "            H[j+1,j] = torch.linalg.norm(w)\n",
    "            V[:,j+1] = w / H[j+1,j]\n",
    "\n",
    "        Vm1 = V\n",
    "        barHm = H\n",
    "        return Vm1, barHm\n",
    "            \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "    # Test Arnoldi\n",
    "    #from GNP.problems import gen_1d_laplacian\n",
    "    #A = gen_1d_laplacian(1000)\n",
    "    #arnoldi = Arnoldi()\n",
    "    #Vm1, barHm = arnoldi.build(A)\n",
    "    #print( torch.linalg.norm( A @ Vm1[:,:-1] - Vm1 @ barHm ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_A_by_spectral_radius(A):\n",
    "    if A.layout == torch.sparse_csc:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        m, n = absA.shape\n",
    "        row_sum = absA @ torch.ones(n, 1, dtype=A.dtype, device=A.device)\n",
    "        col_sum = torch.ones(1, m, dtype=A.dtype, device=A.device) @ absA\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A * (1. / gamma.item())\n",
    "\n",
    "    elif A.layout == torch.strided:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        row_sum = torch.sum(absA, dim=1)\n",
    "        col_sum = torch.sum(absA, dim=0)\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A / gamma\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise NotImplementedError(\n",
    "            'A must be either torch.sparse_csc_tensor or torch.tensor')\n",
    "\n",
    "    return outA\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# An MLP layer.\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num_layers, hidden, drop_rate,\n",
    "                 use_batchnorm=False, is_output_layer=False, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.is_output_layer = is_output_layer\n",
    "\n",
    "        self.lin = nn.ModuleList()\n",
    "        self.lin.append(nn.Linear(in_dim, hidden, dtype=dtype))\n",
    "        for i in range(1, num_layers - 1):\n",
    "            self.lin.append(nn.Linear(hidden, hidden, dtype=dtype))\n",
    "        self.lin.append(nn.Linear(hidden, out_dim, dtype=dtype))\n",
    "        if use_batchnorm:\n",
    "            self.batchnorm = nn.ModuleList()\n",
    "            for i in range(0, num_layers - 1):\n",
    "                self.batchnorm.append(nn.BatchNorm1d(hidden, dtype=dtype))\n",
    "            if not is_output_layer:\n",
    "                self.batchnorm.append(nn.BatchNorm1d(out_dim, dtype=dtype))\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, R):  # R: (*, in_dim)\n",
    "        assert len(R.shape) >= 2\n",
    "        for i in range(self.num_layers):\n",
    "            R = self.lin[i](R)  # (*, hidden)\n",
    "            if i != self.num_layers - 1 or not self.is_output_layer:\n",
    "                if self.use_batchnorm:\n",
    "                    shape = R.shape\n",
    "                    R = R.view(-1, shape[-1])\n",
    "                    R = self.batchnorm[i](R)\n",
    "                    R = R.view(shape)\n",
    "                R = self.dropout(F.relu(R))\n",
    "                # (*, out_dim)\n",
    "        return R\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# A GCN layer.\n",
    "class GCNConv(nn.Module):\n",
    "\n",
    "    def __init__(self, AA, in_dim, out_dim, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        self.AA = AA  # normalized A\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.fc = nn.Linear(in_dim, out_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, R):  # R: (n, batch_size, in_dim)\n",
    "        assert len(R.shape) == 3\n",
    "        n, batch_size, in_dim = R.shape\n",
    "        assert in_dim == self.in_dim\n",
    "        if in_dim > self.out_dim:\n",
    "            R = self.fc(R)  # (n, batch_size, out_dim)\n",
    "            R = R.view(n, batch_size * self.out_dim)  # (n, batch_size * out_dim)\n",
    "            R = self.AA @ R  # (n, batch_size * out_dim)\n",
    "            R = R.view(n, batch_size, self.out_dim)  # (n, batch_size, out_dim)\n",
    "        else:\n",
    "            R = R.view(n, batch_size * in_dim)  # (n, batch_size * in_dim)\n",
    "            R = self.AA @ R  # (n, batch_size * in_dim)\n",
    "            R = R.view(n, batch_size, in_dim)  # (n, batch_size, in_dim)\n",
    "            R = self.fc(R)  # (n, batch_size, out_dim)\n",
    "        return R\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GCN with residual connections.\n",
    "class ResGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, A, num_layers, embed, hidden, drop_rate,\n",
    "                 scale_input=True, dtype=torch.float64):\n",
    "        # A: float64, already on device.\n",
    "        #\n",
    "        # For graph convolution, A will be normalized and cast to\n",
    "        # lower precision and named AA.\n",
    "\n",
    "        super().__init__()\n",
    "        self.dtype = dtype  # used by GNP.precond.GNP\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = embed\n",
    "        self.scale_input = scale_input\n",
    "\n",
    "        # Note: scale_A_by_spectral_radius() has been called when\n",
    "        # defining the problem; hence, it is redundant. We keep the\n",
    "        # code here to leave open the possibility of normalizing A in\n",
    "        # another manner.\n",
    "        self.AA = A.to(dtype)\n",
    "\n",
    "        self.mlp_initial = MLP(1, embed, 4, hidden, drop_rate)\n",
    "        self.mlp_final = MLP(embed, 1, 4, hidden, drop_rate,\n",
    "                             is_output_layer=True, dtype=dtype)\n",
    "        self.gconv = nn.ModuleList()\n",
    "        self.skip = nn.ModuleList()\n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.gconv.append(GCNConv(self.AA, embed, embed, dtype=dtype))\n",
    "            self.skip.append(nn.Linear(embed, embed, dtype=dtype))\n",
    "            self.batchnorm.append(nn.BatchNorm1d(embed, dtype=dtype))\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, r):  # r: (n, batch_size)\n",
    "        assert len(r.shape) == 2\n",
    "        n, batch_size = r.shape\n",
    "        if self.scale_input:\n",
    "            scaling = torch.linalg.vector_norm(r, dim=0) / np.sqrt(n)\n",
    "            r = r / scaling  # scaling\n",
    "        r = r.view(n, batch_size, 1)  # (n, batch_size, 1)\n",
    "        R = self.mlp_initial(r)  # (n, batch_size, embed)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            R = self.gconv[i](R) + self.skip[i](R)  # (n, batch_size, embed)\n",
    "            R = R.view(n * batch_size, self.embed)  # (n * batch_size, embed)\n",
    "            R = self.batchnorm[i](R)  # (n * batch_size, embed)\n",
    "            R = R.view(n, batch_size, self.embed)  # (n, batch_size, embed)\n",
    "            R = self.dropout(F.relu(R))  # (n, batch_size, embed)\n",
    "\n",
    "        z = self.mlp_final(R)  # (n, batch_size, 1)\n",
    "        z = z.view(n, batch_size)  # (n, batch_size)\n",
    "        if self.scale_input:\n",
    "            z = z * scaling  # scaling back\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "#from GNP import GNP\n",
    "#from ResGCN import ResGCN\n",
    "#from GMRES import GMRES\n",
    "#import torch\n",
    "\n",
    "n = 1 # Filter order\n",
    "device = torch.device(\"cuda\")\n",
    "solver = GMRES()\n",
    "tmp = torch.tensor(np.array(ss) * (1.0 / np.square(Kc[-1])), device=device)\n",
    "Smat1 = csc_matrix((ss * (1.0 / jnp.square(Kc[-1])), (ii, jj)), shape=(n2d, n2d))\n",
    "Smat = identity(n2d) + 0.5 * (Smat1 ** n)\n",
    "\n",
    "#Solve the problem\n",
    "\n",
    "def jax_compute(ss, ii, jj, n2d, n, kl, ttu, tol=1e-6, maxiter=150000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the filtered data using a specified filter size.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    n : int\n",
    "        Order of filter, one is recommended\n",
    "\n",
    "    k : float\n",
    "        Wavelength of the filter.\n",
    "\n",
    "    ttu : np.ndarray\n",
    "        NumPy array containing data to be filtered.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        NumPy array with filtered data.\n",
    "    \"\"\"\n",
    "    Smat1 = csc_matrix((ss * (1.0 / jnp.square(kl)), (ii, jj)), shape=(n2d, n2d))\n",
    "    Smat = identity(n2d) + 0.5 * (Smat1 ** n)\n",
    "\n",
    "    b = 1./ Smat.diagonal() # Simple preconditioner\n",
    "    pre = csc_matrix((b, (np.arange(n2d), np.arange(n2d))), shape=(n2d, n2d))\n",
    "    ttw = ttu - Smat @ ttu  # Work with perturbations\n",
    "    iters = 0\n",
    "    # Count number of iterations\n",
    "    def nonlocal_iterate(arr):\n",
    "        nonlocal iters\n",
    "        iters+=1\n",
    "\n",
    "    tts, code = cg(Smat, ttw, rtol=tol, maxiter=maxiter, M=pre, callback=nonlocal_iterate)\n",
    "    if code != 0:\n",
    "        print(\"Solver has not converged terms\")\n",
    "\n",
    "    print(\"Jax Iteration \" + str(iters))\n",
    "\n",
    "    tts += ttu\n",
    "    return np.array(tts)\n",
    "\n",
    "%time\n",
    "tts = jax_compute(ss, ii, jj, n2d, 1, Kc[-1], tt)\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "triang = mpl.tri.Triangulation(xcoord, ycoord, tri)\n",
    "\n",
    "plt.tripcolor(triang, tt)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tripcolor(triang, tts)\n",
    "plt.colorbar()\n",
    "\n",
    "num_layers = 8              # number of layers in GNP\n",
    "embed = 32                  # embedding dimension in GNP\n",
    "hidden = 64                 # hidden dimension in MLPs in GNP\n",
    "drop_rate = 0.1             # dropout rate in GNP\n",
    "disable_scale_input = False # whether disable the scaling of inputs in GNP\n",
    "dtype = torch.float64       # training precision for GNP\n",
    "lr = 5e-3                   # learning rate in training GNP\n",
    "weight_decay = 1e-4         # weight decay in training GNP\n",
    "training_data = 'x_subspace'# type of training data x\n",
    "m = 120                     # Krylov subspace dimension for training data\n",
    "batch_size = 64             # batch size in training GNP\n",
    "grad_accu_steps = 2         # gradient accumulation steps in training GNP\n",
    "epochs = 3000               # number of epochs in training GNP\n",
    "\n",
    "# Start a new run\n",
    "import time\n",
    "run_name = f\"gnp-run-{time.strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"GNP-training\",\n",
    "    entity=\"naomi-rill-alfred-wegener-institut\",  # wandb user name\n",
    "    name=run_name,  \n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"embed\": embed,\n",
    "        \"hidden\": hidden,\n",
    "        \"drop_rate\": drop_rate,\n",
    "        \"training_data\": training_data,\n",
    "        \"krylov_dim\": m,\n",
    "    }\n",
    ")\n",
    "\n",
    "A = torch.sparse_csc_tensor(Smat.indptr, Smat.indices, Smat.data, Smat.shape, dtype=torch.float64).to(device)\n",
    "net = ResGCN(A, num_layers, embed, hidden, drop_rate, scale_input=True, dtype=dtype).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = None\n",
    "M = GNP(A, training_data, m, net, device)\n",
    "\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "hist_loss, best_loss, best_epoch, model_file = M.train(\n",
    "    batch_size, grad_accu_steps, epochs, optimizer, scheduler, num_workers=4,\n",
    "    checkpoint_prefix_with_path=\"./tmp_\", progress_bar=True)\n",
    "\n",
    "print(f'Done. Training time: {time.time()-tic} seconds')\n",
    "print(f'Loss: inital = {hist_loss[0]}, final = {hist_loss[-1]}, best = {best_loss}, epoch = {best_epoch}')\n",
    "print(f'Best model saved in {model_file}')\n",
    "\n",
    "print(f'Loading model from {model_file} ...')\n",
    "net.load_state_dict(torch.load(model_file, map_location=device, weights_only=True))\n",
    "M = GNP(A, training_data, m, net, device)\n",
    "print('Done.')\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "ttt = torch.tensor(np.array(tt), device=device, dtype=torch.float64)\n",
    "\n",
    "#No Preconditioner\n",
    "\n",
    "%time\n",
    "ttw = ttt - A @ ttt  # Work with perturbations\n",
    "\n",
    "x, iters, _, _, _ = solver.solve(A=A, b=ttw, rtol=1e-6, max_iters=20000, progress_bar=False)\n",
    "print(\"Classic Iteration \" + str(iters))\n",
    "x += ttt\n",
    "tts = x.cpu().numpy()\n",
    "\n",
    "#With Preconditioner\n",
    "\n",
    "%time\n",
    "ttw = ttt - A @ ttt  # Work with perturbations\n",
    "\n",
    "x, iters, _, _, _ = solver.solve(A=A, b=ttw, rtol=1e-6, max_iters=20000, progress_bar=False, M=M)\n",
    "print(\"Preconditioned Iteration \" + str(iters))\n",
    "x += ttt\n",
    "tts = x.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NaomiKernel)",
   "language": "python",
   "name": "naomikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
