{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23e2d1e-20d0-4fc7-b2c1-4e65a286397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nbimporter --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6533bbc-07b8-4b92-89ae-ac0280b3ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import GNP\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "# Convention: M \\approx inv(A)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# (Right-)Preconditioned generalized minimal residual method. Can use\n",
    "# flexible preconditioner. Current implementation supports only a\n",
    "# single right-hand side.\n",
    "#\n",
    "# If timeout is not None, max_iters is disabled.\n",
    "class GMRES():\n",
    "\n",
    "    def solve(self, A, b, M=None, x0=None, restart=10, max_iters=100,\n",
    "              timeout=None, rtol=1e-8, progress_bar=True):\n",
    "\n",
    "        if progress_bar:\n",
    "            if timeout is None:\n",
    "                pbar = tqdm(total=max_iters, desc='Solve')\n",
    "                pbar.update()\n",
    "            else:\n",
    "                pbar = tqdm(desc='Solve')\n",
    "                pbar.update()\n",
    "                        \n",
    "        if x0 is None:\n",
    "            x = torch.zeros_like(b)\n",
    "        else:\n",
    "            x = x0\n",
    "        norm_b = torch.linalg.norm(b)\n",
    "        hist_abs_res = []\n",
    "        hist_rel_res = []\n",
    "        hist_time = []\n",
    "\n",
    "        n = len(b)\n",
    "        V = torch.zeros(n, restart+1, dtype=b.dtype).to(b.device)\n",
    "        Z = torch.zeros(n, restart, dtype=b.dtype).to(b.device)\n",
    "        H = torch.zeros(restart+1, restart, dtype=b.dtype).to(b.device)\n",
    "        g = torch.zeros(restart+1, dtype=b.dtype).to(b.device)\n",
    "        c = torch.zeros(restart, dtype=b.dtype).to(b.device)\n",
    "        s = torch.zeros(restart, dtype=b.dtype).to(b.device)\n",
    "        \n",
    "        tic = time.time()\n",
    "\n",
    "        # Initial step\n",
    "        r = b - A @ x\n",
    "        beta = torch.linalg.norm(r)\n",
    "        abs_res = beta\n",
    "        rel_res = abs_res / norm_b\n",
    "        hist_abs_res.append(abs_res.item())\n",
    "        hist_rel_res.append(rel_res.item())\n",
    "        hist_time.append(time.time() - tic)\n",
    "        iters = 0\n",
    "        quit_iter = False\n",
    "\n",
    "        # Outer loop\n",
    "        while 1:\n",
    "\n",
    "            # Restart cycle\n",
    "            V[:,0] = r / beta\n",
    "            g[0] = beta\n",
    "            for j in range(restart):\n",
    "                if M is not None:\n",
    "                    Z[:,j] = M.apply(V[:,j])\n",
    "                else:\n",
    "                    Z[:,j] = V[:,j]\n",
    "                w = A @ Z[:,j]\n",
    "                for k in range(j+1):\n",
    "                    H[k,j] = torch.dot(V[:,k], w)\n",
    "                    w = w - H[k,j] * V[:,k]\n",
    "                H[j+1,j] = torch.linalg.norm(w)\n",
    "                V[:,j+1] = w / H[j+1,j]\n",
    "\n",
    "                # Solve min || H * y - beta * e1 ||: Givens rotation\n",
    "                for k in range(j):\n",
    "                    tmp      =  c[k] * H[k,j] + s[k] * H[k+1,j]\n",
    "                    H[k+1,j] = -s[k] * H[k,j] + c[k] * H[k+1,j]\n",
    "                    H[k,j] = tmp\n",
    "                t = torch.sqrt( H[j,j]**2 + H[j+1,j]**2 )\n",
    "                c[j], s[j] = H[j,j]/t, H[j+1,j]/t\n",
    "                H[j,j] = c[j] * H[j,j] + s[j] * H[j+1,j]\n",
    "                H[j+1,j] = 0\n",
    "                g[j+1] = -s[j] * g[j]\n",
    "                g[j] = c[j] * g[j]\n",
    "                # End solve min || H * y - beta * e1 ||: Givens rotation\n",
    "\n",
    "                abs_res = torch.abs(g[j+1])\n",
    "                rel_res = abs_res / norm_b\n",
    "                hist_abs_res.append(abs_res.item())\n",
    "                hist_rel_res.append(rel_res.item())\n",
    "                hist_time.append(time.time() - tic)\n",
    "                iters = iters + 1\n",
    "                if (rel_res < rtol) or \\\n",
    "                   (timeout is None and iters == max_iters) or \\\n",
    "                   (timeout is not None and hist_time[-1] >= timeout):\n",
    "                    quit_iter = True\n",
    "                    break\n",
    "\n",
    "                if progress_bar:\n",
    "                    pbar.update()\n",
    "            # End restart cycle\n",
    "\n",
    "            # Solve min || H * y - beta * e1 ||: obtain solution\n",
    "            y = torch.linalg.solve_triangular(H[:j+1, :j+1],\n",
    "                                              g[:j+1].view(j+1,1),\n",
    "                                              upper=True).view(j+1)\n",
    "            # End solve min || H * y - beta * e1 ||: obtain solution\n",
    "\n",
    "            x = x + Z[:, :j+1] @ y\n",
    "            r = b - A @ x\n",
    "            beta = torch.linalg.norm(r)\n",
    "            if np.allclose(hist_abs_res[-1], beta.item()) is False:\n",
    "                warn('Residual tracked by least squares solve is different '\n",
    "                     'from the true residual. The result of GMRES should not '\n",
    "                     'be trusted.')\n",
    "            if quit_iter == True:\n",
    "                break\n",
    "            else:\n",
    "                g = g.zero_()\n",
    "                g[0] = beta\n",
    "        # End outer loop\n",
    "\n",
    "        if progress_bar:\n",
    "            pbar.close()\n",
    "\n",
    "        return x, iters, hist_abs_res, hist_rel_res, hist_time\n",
    "            \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Arnoldi process (m steps).\n",
    "class Arnoldi():\n",
    "\n",
    "    def build(self, A, v0=None, m=100):\n",
    "\n",
    "        n = A.shape[0]\n",
    "        if v0 is None:\n",
    "            v0 = torch.normal(0, 1, size=(n,), dtype=A.dtype).to(A.device)\n",
    "        beta = torch.linalg.norm(v0)\n",
    "        \n",
    "        V = torch.zeros(n, m+1, dtype=A.dtype).to(A.device)\n",
    "        H = torch.zeros(m+1, m, dtype=A.dtype).to(A.device)\n",
    "\n",
    "        V[:,0] = v0 / beta\n",
    "        for j in range(m):\n",
    "            w = A @ V[:,j]\n",
    "            for k in range(j+1):\n",
    "                H[k,j] = torch.dot(V[:,k], w)\n",
    "                w = w - H[k,j] * V[:,k]\n",
    "            H[j+1,j] = torch.linalg.norm(w)\n",
    "            V[:,j+1] = w / H[j+1,j]\n",
    "\n",
    "        Vm1 = V\n",
    "        barHm = H\n",
    "        return Vm1, barHm\n",
    "            \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "    # Test Arnoldi\n",
    "    #from GNP.problems import gen_1d_laplacian\n",
    "    #A = gen_1d_laplacian(1000)\n",
    "    #arnoldi = Arnoldi()\n",
    "    #Vm1, barHm = arnoldi.build(A)\n",
    "    #print( torch.linalg.norm( A @ Vm1[:,:-1] - Vm1 @ barHm ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d841f03-27ce-4a64-a42b-14b710e25ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NaomiKernel)",
   "language": "python",
   "name": "naomikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
