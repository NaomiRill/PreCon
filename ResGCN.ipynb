{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9072c2ef-04af-45b5-afe7-75e5dd8864e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_A_by_spectral_radius(A):\n",
    "    if A.layout == torch.sparse_csc:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        m, n = absA.shape\n",
    "        row_sum = absA @ torch.ones(n, 1, dtype=A.dtype, device=A.device)\n",
    "        col_sum = torch.ones(1, m, dtype=A.dtype, device=A.device) @ absA\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A * (1. / gamma.item())\n",
    "\n",
    "    elif A.layout == torch.strided:\n",
    "\n",
    "        absA = torch.absolute(A)\n",
    "        row_sum = torch.sum(absA, dim=1)\n",
    "        col_sum = torch.sum(absA, dim=0)\n",
    "        gamma = torch.min(torch.max(row_sum), torch.max(col_sum))\n",
    "        outA = A / gamma\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise NotImplementedError(\n",
    "            'A must be either torch.sparse_csc_tensor or torch.tensor')\n",
    "\n",
    "    return outA\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# An MLP layer.\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num_layers, hidden, drop_rate,\n",
    "                 use_batchnorm=False, is_output_layer=False, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.is_output_layer = is_output_layer\n",
    "\n",
    "        self.lin = nn.ModuleList()\n",
    "        self.lin.append(nn.Linear(in_dim, hidden, dtype=dtype))\n",
    "        for i in range(1, num_layers - 1):\n",
    "            self.lin.append(nn.Linear(hidden, hidden, dtype=dtype))\n",
    "        self.lin.append(nn.Linear(hidden, out_dim, dtype=dtype))\n",
    "        if use_batchnorm:\n",
    "            self.batchnorm = nn.ModuleList()\n",
    "            for i in range(0, num_layers - 1):\n",
    "                self.batchnorm.append(nn.BatchNorm1d(hidden, dtype=dtype))\n",
    "            if not is_output_layer:\n",
    "                self.batchnorm.append(nn.BatchNorm1d(out_dim, dtype=dtype))\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, R):  # R: (*, in_dim)\n",
    "        assert len(R.shape) >= 2\n",
    "        for i in range(self.num_layers):\n",
    "            R = self.lin[i](R)  # (*, hidden)\n",
    "            if i != self.num_layers - 1 or not self.is_output_layer:\n",
    "                if self.use_batchnorm:\n",
    "                    shape = R.shape\n",
    "                    R = R.view(-1, shape[-1])\n",
    "                    R = self.batchnorm[i](R)\n",
    "                    R = R.view(shape)\n",
    "                R = self.dropout(F.relu(R))\n",
    "                # (*, out_dim)\n",
    "        return R\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# A GCN layer.\n",
    "class GCNConv(nn.Module):\n",
    "\n",
    "    def __init__(self, AA, in_dim, out_dim, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        self.AA = AA  # normalized A\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.fc = nn.Linear(in_dim, out_dim, dtype=dtype)\n",
    "\n",
    "    def forward(self, R):  # R: (n, batch_size, in_dim)\n",
    "        assert len(R.shape) == 3\n",
    "        n, batch_size, in_dim = R.shape\n",
    "        assert in_dim == self.in_dim\n",
    "        if in_dim > self.out_dim:\n",
    "            R = self.fc(R)  # (n, batch_size, out_dim)\n",
    "            R = R.view(n, batch_size * self.out_dim)  # (n, batch_size * out_dim)\n",
    "            R = self.AA @ R  # (n, batch_size * out_dim)\n",
    "            R = R.view(n, batch_size, self.out_dim)  # (n, batch_size, out_dim)\n",
    "        else:\n",
    "            R = R.view(n, batch_size * in_dim)  # (n, batch_size * in_dim)\n",
    "            R = self.AA @ R  # (n, batch_size * in_dim)\n",
    "            R = R.view(n, batch_size, in_dim)  # (n, batch_size, in_dim)\n",
    "            R = self.fc(R)  # (n, batch_size, out_dim)\n",
    "        return R\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GCN with residual connections.\n",
    "class ResGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, A, num_layers, embed, hidden, drop_rate,\n",
    "                 scale_input=True, dtype=torch.float64):\n",
    "        # A: float64, already on device.\n",
    "        #\n",
    "        # For graph convolution, A will be normalized and cast to\n",
    "        # lower precision and named AA.\n",
    "\n",
    "        super().__init__()\n",
    "        self.dtype = dtype  # used by GNP.precond.GNP\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = embed\n",
    "        self.scale_input = scale_input\n",
    "\n",
    "        # Note: scale_A_by_spectral_radius() has been called when\n",
    "        # defining the problem; hence, it is redundant. We keep the\n",
    "        # code here to leave open the possibility of normalizing A in\n",
    "        # another manner.\n",
    "        self.AA = A.to(dtype)\n",
    "\n",
    "        self.mlp_initial = MLP(1, embed, 4, hidden, drop_rate)\n",
    "        self.mlp_final = MLP(embed, 1, 4, hidden, drop_rate,\n",
    "                             is_output_layer=True, dtype=dtype)\n",
    "        self.gconv = nn.ModuleList()\n",
    "        self.skip = nn.ModuleList()\n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.gconv.append(GCNConv(self.AA, embed, embed, dtype=dtype))\n",
    "            self.skip.append(nn.Linear(embed, embed, dtype=dtype))\n",
    "            self.batchnorm.append(nn.BatchNorm1d(embed, dtype=dtype))\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, r):  # r: (n, batch_size)\n",
    "        assert len(r.shape) == 2\n",
    "        n, batch_size = r.shape\n",
    "        if self.scale_input:\n",
    "            scaling = torch.linalg.vector_norm(r, dim=0) / np.sqrt(n)\n",
    "            r = r / scaling  # scaling\n",
    "        r = r.view(n, batch_size, 1)  # (n, batch_size, 1)\n",
    "        R = self.mlp_initial(r)  # (n, batch_size, embed)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            R = self.gconv[i](R) + self.skip[i](R)  # (n, batch_size, embed)\n",
    "            R = R.view(n * batch_size, self.embed)  # (n * batch_size, embed)\n",
    "            R = self.batchnorm[i](R)  # (n * batch_size, embed)\n",
    "            R = R.view(n, batch_size, self.embed)  # (n, batch_size, embed)\n",
    "            R = self.dropout(F.relu(R))  # (n, batch_size, embed)\n",
    "\n",
    "        z = self.mlp_final(R)  # (n, batch_size, 1)\n",
    "        z = z.view(n, batch_size)  # (n, batch_size)\n",
    "        if self.scale_input:\n",
    "            z = z * scaling  # scaling back\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e10190-2d50-4b6f-987d-9aaef4d37b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NaomiKernel)",
   "language": "python",
   "name": "naomikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
